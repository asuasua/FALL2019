{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EXT_SOURCE_3</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_MON</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_QRT</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_YEAR</th>\n",
       "      <th>OBS_30_CNT_SOCIAL_CIRCLE</th>\n",
       "      <th>DEF_30_CNT_SOCIAL_CIRCLE</th>\n",
       "      <th>OBS_60_CNT_SOCIAL_CIRCLE</th>\n",
       "      <th>DEF_60_CNT_SOCIAL_CIRCLE</th>\n",
       "      <th>EXT_SOURCE_2</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>...</th>\n",
       "      <th>REGION_RATING_CLIENT</th>\n",
       "      <th>REGION_RATING_CLIENT_W_CITY</th>\n",
       "      <th>WEEKDAY_APPR_PROCESS_START</th>\n",
       "      <th>HOUR_APPR_PROCESS_START</th>\n",
       "      <th>REG_REGION_NOT_LIVE_REGION</th>\n",
       "      <th>REG_REGION_NOT_WORK_REGION</th>\n",
       "      <th>LIVE_REGION_NOT_WORK_REGION</th>\n",
       "      <th>REG_CITY_NOT_LIVE_CITY</th>\n",
       "      <th>REG_CITY_NOT_WORK_CITY</th>\n",
       "      <th>LIVE_CITY_NOT_WORK_CITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.905793</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.351241</td>\n",
       "      <td>-0.527764</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>WEDNESDAY</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.123108</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.197991</td>\n",
       "      <td>-1.109049</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>MONDAY</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.567098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.868560</td>\n",
       "      <td>-0.249232</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>WEDNESDAY</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.095792</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.086863</td>\n",
       "      <td>2.281779</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>SUNDAY</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.153597</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.035459</td>\n",
       "      <td>2.645082</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>MONDAY</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   EXT_SOURCE_3  AMT_REQ_CREDIT_BUREAU_MON  AMT_REQ_CREDIT_BUREAU_QRT  \\\n",
       "0     -1.905793                        0.0                        0.0   \n",
       "1      1.123108                        0.0                        0.0   \n",
       "2      0.567098                        0.0                        1.0   \n",
       "3     -0.095792                        1.0                        1.0   \n",
       "4      0.153597                        0.0                        0.0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_YEAR  OBS_30_CNT_SOCIAL_CIRCLE  \\\n",
       "0                         1.0                       2.0   \n",
       "1                         0.0                       0.0   \n",
       "2                         1.0                       0.0   \n",
       "3                         2.0                       1.0   \n",
       "4                         0.0                       2.0   \n",
       "\n",
       "   DEF_30_CNT_SOCIAL_CIRCLE  OBS_60_CNT_SOCIAL_CIRCLE  \\\n",
       "0                       2.0                       2.0   \n",
       "1                       0.0                       0.0   \n",
       "2                       0.0                       0.0   \n",
       "3                       0.0                       1.0   \n",
       "4                       0.0                       2.0   \n",
       "\n",
       "   DEF_60_CNT_SOCIAL_CIRCLE  EXT_SOURCE_2  AMT_GOODS_PRICE  ...  \\\n",
       "0                       2.0     -1.351241        -0.527764  ...   \n",
       "1                       0.0      0.197991        -1.109049  ...   \n",
       "2                       0.0     -0.868560        -0.249232  ...   \n",
       "3                       0.0      1.086863         2.281779  ...   \n",
       "4                       0.0      1.035459         2.645082  ...   \n",
       "\n",
       "   REGION_RATING_CLIENT  REGION_RATING_CLIENT_W_CITY  \\\n",
       "0                     2                            2   \n",
       "1                     2                            2   \n",
       "2                     2                            2   \n",
       "3                     2                            2   \n",
       "4                     3                            3   \n",
       "\n",
       "   WEEKDAY_APPR_PROCESS_START  HOUR_APPR_PROCESS_START  \\\n",
       "0                   WEDNESDAY                       10   \n",
       "1                      MONDAY                        9   \n",
       "2                   WEDNESDAY                       16   \n",
       "3                      SUNDAY                       16   \n",
       "4                      MONDAY                       16   \n",
       "\n",
       "  REG_REGION_NOT_LIVE_REGION REG_REGION_NOT_WORK_REGION  \\\n",
       "0                          0                          0   \n",
       "1                          0                          0   \n",
       "2                          0                          0   \n",
       "3                          0                          0   \n",
       "4                          0                          0   \n",
       "\n",
       "   LIVE_REGION_NOT_WORK_REGION  REG_CITY_NOT_LIVE_CITY  \\\n",
       "0                            0                       0   \n",
       "1                            0                       0   \n",
       "2                            0                       0   \n",
       "3                            0                       0   \n",
       "4                            0                       0   \n",
       "\n",
       "   REG_CITY_NOT_WORK_CITY  LIVE_CITY_NOT_WORK_CITY  \n",
       "0                       0                        0  \n",
       "1                       0                        0  \n",
       "2                       0                        0  \n",
       "3                       0                        0  \n",
       "4                       1                        1  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"impot data select features\"\"\"\n",
    "data = pd.read_csv(\"application_training_data_import.csv\")\n",
    "features = data.columns\n",
    "s=data.isna().sum()\n",
    "null = pd.read_csv('Null_Entries.csv')\n",
    "d = null[null['flag']==0]\n",
    "columns = d['colname']\n",
    "new_data = data[columns]\n",
    "\n",
    "\n",
    "\n",
    "home_loans = new_data[new_data.isnull().any(axis=1) == False]\n",
    "home_loans = home_loans.reset_index()\n",
    "home_loans = home_loans.drop(columns = 'index')\n",
    "\n",
    "#print(home_loans.shape)\n",
    "\n",
    "\n",
    "cols_to_remove = ['SK_ID_CURR',\n",
    "'FLAG_DOCUMENT_2',\n",
    "'FLAG_DOCUMENT_3',\n",
    "'FLAG_DOCUMENT_4',\n",
    "'FLAG_DOCUMENT_5',\n",
    "'FLAG_DOCUMENT_6',  \n",
    "'FLAG_DOCUMENT_7',\n",
    "'FLAG_DOCUMENT_8',\n",
    "'FLAG_DOCUMENT_9',\n",
    "'FLAG_DOCUMENT_10',\n",
    "'FLAG_DOCUMENT_11',\n",
    "'FLAG_DOCUMENT_12',\n",
    "'FLAG_DOCUMENT_13',\n",
    "'FLAG_DOCUMENT_14',\n",
    "'FLAG_DOCUMENT_15',\n",
    "'FLAG_DOCUMENT_16',\n",
    "'FLAG_DOCUMENT_17',\n",
    "'FLAG_DOCUMENT_18',                  \n",
    "'FLAG_DOCUMENT_19',\n",
    "'FLAG_DOCUMENT_20',\n",
    "'FLAG_DOCUMENT_21', \n",
    "'AMT_REQ_CREDIT_BUREAU_HOUR',\n",
    "'AMT_REQ_CREDIT_BUREAU_DAY',\n",
    "'AMT_REQ_CREDIT_BUREAU_WEEK',\n",
    "'FLAG_CONT_MOBILE']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cols_to_normalize = ['EXT_SOURCE_3',\n",
    "'EXT_SOURCE_2',\n",
    "'AMT_GOODS_PRICE',\n",
    "'AMT_ANNUITY',\n",
    "'DAYS_LAST_PHONE_CHANGE', 'AMT_INCOME_TOTAL',\n",
    "'AMT_CREDIT',\n",
    "'REGION_POPULATION_RELATIVE',\n",
    "'DAYS_BIRTH',\n",
    "'DAYS_EMPLOYED',\n",
    "'DAYS_REGISTRATION',\n",
    "'DAYS_ID_PUBLISH']\n",
    "\n",
    "\n",
    "\"\"\"drop columns and add normalized columns\"\"\"\n",
    "home_loans_df = home_loans.drop(cols_to_remove, axis=1)\n",
    "## substitute the normalized columns\n",
    "df_to_normalize = home_loans_df[cols_to_normalize]\n",
    "norm_values = df_to_normalize.values\n",
    "\n",
    "\"\"\"normalize values\"\"\"\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(norm_values)\n",
    "norm_values= scaler.transform(norm_values)\n",
    "norm_values\n",
    "\n",
    "\"\"\"re-impute into data\"\"\"\n",
    "home_loans_df[cols_to_normalize] = norm_values\n",
    "home_loans_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(home_loans_df[home_loans_df['TARGET']==1]))\n",
    "# print(len(home_loans_df[home_loans_df['TARGET']==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(home_loans['TARGET'].value_counts()[0]/len(home_loans), home_loans['TARGET'].value_counts()[1]/len(home_loans))\n",
    "# print(home_loans.shape)\n",
    "# home_loans = home_loans.drop_duplicates()\n",
    "# print(home_loans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shaped (22000, 67)\n",
      "(7333, 67)\n",
      "(7333, 67)\n",
      "(7334, 67)\n",
      "Testing data shaped (20290, 67)\n",
      "cross validation shapes\n",
      "(14666, 67)\n",
      "(14667, 67)\n",
      "(14667, 67)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Set the indices\"\"\"\n",
    "n0 = home_loans_df[home_loans_df['TARGET']==0].shape[0]\n",
    "#test set percentage size \n",
    "test_per = 0.1\n",
    "n1 = home_loans_df[home_loans_df['TARGET']==1].shape[0]\n",
    "extra = 0\n",
    "\n",
    "train1_index = 12000\n",
    "train0_index = int(train1)\n",
    "\n",
    "test1_index = int(n1 - train1)\n",
    "test0_index = int(test1/per1) + 0\n",
    "\n",
    "\n",
    "\"\"\"Take all TARGET Home Loans\"\"\"\n",
    "home_loans = shuffle(home_loans_df,random_state=0)\n",
    "home_loans0 = home_loans[home_loans['TARGET']==0].iloc[:101000, :]\n",
    "home_loans1 = home_loans[home_loans['TARGET']==1].iloc[:, :]\n",
    "\n",
    "# home_loans_test0 = home_loans[home_loans['TARGET']==0].iloc[10000:101000, :]\n",
    "# home_loans_test1 = home_loans[home_loans['TARGET']==1].iloc[:, :]\n",
    "\n",
    "reduced_loans = pd.concat([home_loans0, home_loans1], axis=0)\n",
    "reduced_loans = shuffle(reduced_loans,random_state=0)\n",
    "\n",
    "\n",
    "reduced_loans = pd.get_dummies(reduced_loans)\n",
    "\n",
    "\n",
    "##########\n",
    "################\n",
    "########---Split data into Test-Train Split\n",
    "####################\n",
    "########################\n",
    "\"\"\"split dataset into training set and cross validation\"\"\"\n",
    "\"\"\"training data will have 50/50 0/1 class split\"\"\"\n",
    "train_df0 = reduced_loans[reduced_loans['TARGET']==0].iloc[:train0_index, :]\n",
    "train_df1 = reduced_loans[reduced_loans['TARGET']==1].iloc[:train1_index, :]\n",
    "\n",
    "train = pd.concat([train_df0, train_df1])\n",
    "train = shuffle(train,random_state=43)\n",
    "\n",
    "\n",
    "\"\"\"Test dataframe will have 90/10 0/1 class split\"\"\"\n",
    "test_df0 = reduced_loans[reduced_loans['TARGET']==0].iloc[test0_index:, :]\n",
    "test_df1 = reduced_loans[reduced_loans['TARGET']==1].iloc[test1_index:, :]\n",
    "\n",
    "test = pd.concat([test_df0, test_df1])\n",
    "test = shuffle(test,random_state=57)\n",
    "\n",
    "del reduced_loans, home_loans, home_loans0, home_loans1\n",
    "######\n",
    "#########\n",
    "#######-----split up the y variables\n",
    "##########\n",
    "\n",
    "\n",
    "\"\"\"separate out the target variable\"\"\"\n",
    "train_target = train['TARGET']\n",
    "train = train.drop(columns = 'TARGET')\n",
    "train = pd.get_dummies(train) #One Hot Encoding\n",
    "# train = train.drop(columns='NAME_INCOME_TYPE_Unemployed')\n",
    "# train = train.drop(columns='GENDER_CODE_XNA')\n",
    "\n",
    "test_target = test['TARGET']\n",
    "test = test.drop(columns = 'TARGET')\n",
    "test = pd.get_dummies(test) #One Hot Encoding\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"break up the data\"\"\"\n",
    "X_train, y_train = np.matrix(train), np.array(train_target)\n",
    "X_test, y_test = np.matrix(test), np.array(test_target)\n",
    "\n",
    "#######\n",
    "#######---Create cross validation sets-------######\n",
    "########\n",
    "#########\n",
    "\n",
    "traincv1 = X_train[:int(np.floor((len(train_target)/3))), :]\n",
    "target1 = y_train[:int(np.floor((len(train_target)/3)))]\n",
    "\n",
    "traincv2 = X_train[int(np.floor((len(train_target)/3))):int(np.floor((len(train_target)*2/3))), :]\n",
    "target2 = y_train[int(np.floor((len(train_target)/3))):int(np.floor((len(train_target)*2/3)))]\n",
    "\n",
    "\n",
    "traincv3 = X_train[int(np.floor((len(train_target)*2/3))):, :]\n",
    "target3 = y_train[int(np.floor((len(train_target)*2/3))):]\n",
    "\n",
    "\n",
    "\"\"\"add test dataset\"\"\"\n",
    "# test = X[int(np.floor((len(reduced_loans)*3/4))):, :]\n",
    "# target_test = y[int(np.floor((len(reduced_loans)*3/4))):]\n",
    "\n",
    "\n",
    "\"\"\"Trainig \"\"\"\n",
    "print('Training data shaped {}'.format(X_train.shape))\n",
    "print(traincv1.shape)\n",
    "print(traincv2.shape)\n",
    "print(traincv3.shape)\n",
    "print('Testing data shaped {}'.format(X_test.shape))\n",
    "\n",
    "\n",
    "\"\"\"make pairs of datasets\"\"\"\n",
    "CV12 = np.concatenate([traincv1, traincv2])\n",
    "y12 = np.concatenate([target1, target2])\n",
    "\n",
    "CV13 = np.concatenate([traincv1, traincv3])\n",
    "y13 = np.concatenate([target1, target3])\n",
    "\n",
    "CV23 = np.concatenate([traincv2, traincv3])\n",
    "y23 = np.concatenate([target2, target3])\n",
    "\n",
    "print('cross validation shapes')\n",
    "print(CV12.shape)\n",
    "print(CV13.shape)\n",
    "print(CV23.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### support vector machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Train on 1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training on 1,2\n",
      "Trained on 1,2\n",
      "Train Confusion Matrix with accuracy 0.9218600845492977\n",
      "Train Recall: 0.9475458973398276\n",
      "[[5933  726]\n",
      " [ 420 7587]]\n",
      "\n",
      "Test Confusion Matrix with accuracy 0.6588491955276793\n",
      "Test Recall: 0.7422990232907588\n",
      "[[1868 1473]\n",
      " [1029 2964]]\n"
     ]
    }
   ],
   "source": [
    "support_vector_machine12 = SVC(kernel='rbf', gamma=0.1, C=1., random_state=None)\n",
    "support_vector_machine12.fit(CV12, y12)\n",
    "print('finished training on 1,2')\n",
    "\n",
    "#y_scores = clf.decision_function(X_test)\n",
    "y_scores_train = support_vector_machine12.predict(CV12)\n",
    "y_scores_test = support_vector_machine12.predict(traincv3)\n",
    "\n",
    "confusion_train = confusion_matrix(y12, y_scores_train)\n",
    "confusion_test = confusion_matrix(target3, y_scores_test)\n",
    "\n",
    "\n",
    "print('Trained on 1,2')\n",
    "print('Train Confusion Matrix with accuracy {}'.format((confusion_train[0][0]+confusion_train[1][1])/len(y12)))\n",
    "print('Train Recall: {}'.format((confusion_train[1][1]/(confusion_train[1][0]+confusion_train[1][1]))))\n",
    "print(confusion_train)\n",
    "print('')\n",
    "print('Test Confusion Matrix with accuracy {}'.format((confusion_test[0][0]+confusion_test[1][1])/len(target3)))\n",
    "print('Test Recall: {}'.format((confusion_test[1][1]/(confusion_test[1][0]+confusion_test[1][1]))))\n",
    "print(confusion_test)\n",
    "#precision, recall, thresholds = precision_recall_curve(y_test, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Train on 1,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training on 1,3\n",
      "Trained on 1,3\n",
      "Train Confusion Matrix with accuracy 0.9224108542987659\n",
      "Train Recall: 0.9387984981226534\n",
      "[[6028  649]\n",
      " [ 489 7501]]\n",
      "\n",
      "Test Confusion Matrix with accuracy 0.6654847947633984\n",
      "Test Recall: 0.729426433915212\n",
      "[[1955 1368]\n",
      " [1085 2925]]\n"
     ]
    }
   ],
   "source": [
    "support_vector_machine13 = SVC(kernel='rbf', gamma=0.1, C=1., random_state=None)\n",
    "support_vector_machine13.fit(CV13, y13)\n",
    "print('finished training on 1,3')\n",
    "\n",
    "#y_scores = clf.decision_function(X_test)\n",
    "y_scores_train = support_vector_machine13.predict(CV13) \n",
    "y_scores_test = support_vector_machine13.predict(traincv2)\n",
    "\n",
    "confusion_train = confusion_matrix(y13, y_scores_train)\n",
    "confusion_test = confusion_matrix(target2, y_scores_test)\n",
    "\n",
    "\n",
    "print('Trained on 1,3')\n",
    "print('Train Confusion Matrix with accuracy {}'.format((confusion_train[0][0]+confusion_train[1][1])/len(y13)))\n",
    "print('Train Recall: {}'.format((confusion_train[1][1]/(confusion_train[1][0]+confusion_train[1][1]))))\n",
    "print(confusion_train)\n",
    "print('')\n",
    "print('Test Confusion Matrix with accuracy {}'.format((confusion_test[0][0]+confusion_test[1][1])/len(target2)))\n",
    "print('Test Recall: {}'.format((confusion_test[1][1]/(confusion_test[1][0]+confusion_test[1][1]))))\n",
    "print(confusion_test)\n",
    "#precision, recall, thresholds = precision_recall_curve(y_test, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Train on 2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training on 2,3\n",
      "Trained on 2,3\n",
      "Train Confusion Matrix with accuracy 0.919001840867253\n",
      "Train Recall: 0.937023616143946\n",
      "[[5980  684]\n",
      " [ 504 7499]]\n",
      "\n",
      "Test Confusion Matrix with accuracy 0.6593481521887359\n",
      "Test Recall: 0.712784588441331\n",
      "[[1986 1350]\n",
      " [1148 2849]]\n"
     ]
    }
   ],
   "source": [
    "support_vector_machine23 = SVC(kernel='rbf', gamma=0.1, C=1., random_state=None)\n",
    "support_vector_machine23.fit(CV23, y23)\n",
    "print('finished training on 2,3')\n",
    "#y_scores = clf.decision_function(X_test)\n",
    "y_scores_train = support_vector_machine23.predict(CV23)\n",
    "y_scores_test = support_vector_machine23.predict(traincv1)\n",
    "\n",
    "confusion_train = confusion_matrix(y23, y_scores_train)\n",
    "confusion_test = confusion_matrix(target1, y_scores_test)\n",
    "\n",
    "\n",
    "print('Trained on 2,3')\n",
    "print('Train Confusion Matrix with accuracy {}'.format((confusion_train[0][0]+confusion_train[1][1])/len(y23)))\n",
    "print('Train Recall: {}'.format((confusion_train[1][1]/(confusion_train[1][0]+confusion_train[1][1]))))\n",
    "print(confusion_train)\n",
    "print('')\n",
    "print('Test Confusion Matrix with accuracy {}'.format((confusion_test[0][0]+confusion_test[1][1])/len(target1)))\n",
    "print('Test Recall: {}'.format((confusion_test[1][1]/(confusion_test[1][0]+confusion_test[1][1]))))\n",
    "print(confusion_test)\n",
    "#precision, recall, thresholds = precision_recall_curve(y_test, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain on Best Model ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on TEST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training on FULL Training Data\n",
      "Trained on FULL Training Data\n",
      "Train Confusion Matrix with accuracy 0.9084090909090909\n",
      "Train Recall: 0.9299166666666666\n",
      "[[ 8826  1174]\n",
      " [  841 11159]]\n",
      "\n",
      "Test Confusion Matrix with accuracy 0.695465746673238\n",
      "Test Recall: 0.8027\n",
      "[[6084 4206]\n",
      " [1973 8027]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"After Eyeballing the model with the best Recall\"\"\"\n",
    "support_vector_machine_best = support_vector_machine12\n",
    "support_vector_machine_best.fit(X_train, y_train)\n",
    "print('finished training on FULL Training Data')\n",
    "#y_scores = clf.decision_function(X_test)\n",
    "y_scores_train = support_vector_machine_best.predict(X_train)\n",
    "y_scores_test = support_vector_machine_best.predict(X_test)\n",
    "\n",
    "confusion_train = confusion_matrix(y_train, y_scores_train)\n",
    "confusion_test = confusion_matrix(y_test, y_scores_test)\n",
    "\n",
    "\n",
    "print('Trained on FULL Training Data')\n",
    "print('Train Confusion Matrix with accuracy {}'.format((confusion_train[0][0]+confusion_train[1][1])/len(y_train)))\n",
    "print('Train Recall: {}'.format((confusion_train[1][1]/(confusion_train[1][0]+confusion_train[1][1]))))\n",
    "print(confusion_train)\n",
    "print('')\n",
    "print('Test Confusion Matrix with accuracy {}'.format((confusion_test[0][0]+confusion_test[1][1])/len(y_test)))\n",
    "print('Test Recall: {}'.format((confusion_test[1][1]/(confusion_test[1][0]+confusion_test[1][1]))))\n",
    "print(confusion_test)\n",
    "#precision, recall, thresholds = precision_recall_curve(y_test, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_values = {'C':[0.01, 0.1, 1, 10], 'gamma':[0.01, 0.1, 1, 10]}\n",
    "# # default metric to optimize over grid parameters: accuracy\n",
    "# grid_clf_acc = GridSearchCV(support_vector_machine, param_grid = grid_values, scoring='accuracy')\n",
    "# #y_decision_fn_scores_acc = grid_clf_acc.decision_function(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"train a model with the best training accuracy\"\"\"\n",
    "# training_accuracy = grid_clf_acc.fit(X_train, y_train)\n",
    "# best_model = training_accuracy.best_estimator_\n",
    "# y_scores = best_model.predict(X_test)\n",
    "# confusion = confusion_matrix(y_test, y_scores)\n",
    "# #precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
    "# print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_scores, test_scores = validation_curve(SVC(), X, y,\n",
    "#                                             param_name='gamma',\n",
    "#                                             param_range=param_range, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.8000)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"split up data into test train spit\"\"\"\n",
    "# X, y = np.matrix(train), np.array(target)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "\n",
    "CV12 = torch.tensor(CV12).float()\n",
    "y12 = torch.tensor(y12).long()\n",
    "CV13 = torch.tensor(CV13).float()\n",
    "y13 = torch.tensor(y13).long()\n",
    "CV23 = torch.tensor(CV23).float()\n",
    "y23 = torch.tensor(y23).long()\n",
    "\n",
    "train = torch.tensor(X_train).float() \n",
    "y_train = torch.tensor(y_train).long()\n",
    "test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).long()\n",
    "\n",
    "\n",
    "\n",
    "traincv1 = torch.tensor(traincv1).float()\n",
    "target1 = torch.tensor(target1).long()\n",
    "traincv2 = torch.tensor(traincv2).float()\n",
    "target2 = torch.tensor(target2).long()\n",
    "traincv3 = torch.tensor(traincv3).float()\n",
    "target3 = torch.tensor(target3).long()\n",
    "print('data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"define some functions\"\"\"\n",
    "def train_neural_network(nn_name, learning_rate, epochs, train_tensor, target_tensor): \n",
    "    \"\"\"declare name\"\"\"\n",
    "    name = nn_name\n",
    "    neural_network = Net(name=name)\n",
    "    optimizer = optim.Adam(neural_network.parameters(), lr=learning_rate) ## Adam optimizer with parameters and learning rate\n",
    "    EPOCHS = epochs\n",
    "    \n",
    "    XX = torch.tensor(train_tensor).float()\n",
    "    target = torch.tensor(target_tensor).long()\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        XX = XX\n",
    "        target = target\n",
    "        \n",
    "        neural_network.zero_grad()\n",
    "        output = neural_network.forward(XX)\n",
    "        predicted = output.float()\n",
    "        \n",
    "        #print(predicted.shape, target.shape)\n",
    "        loss = F.cross_entropy(predicted, target)\n",
    "        #loss = F.nll_loss(output, y) ## calculate loss function with MSE or cross_entropy\n",
    "        #F.cross_entropy\n",
    "        #  F.leaky_relu\n",
    "        loss.backward() ## do the backprop\n",
    "        optimizer.step() ## update the weights\n",
    "        if epoch % 200 == 0:\n",
    "            print('epoch {} had loss {}'.format(epoch, loss))\n",
    "\n",
    "    print('final loss was {}\\n\\n'.format(loss))\n",
    "    \n",
    "    return neural_network\n",
    "\n",
    "\n",
    "\n",
    "#####\n",
    "##########\n",
    "###------------########\n",
    "######-----------------##\n",
    "#####---------------------######\n",
    "\n",
    "\n",
    "def test_scores(neural_network, X_train, y_train, X_test, y_test):\n",
    "    total_train = 0\n",
    "    correct_train = 0\n",
    "    total_test = 0\n",
    "    correct_test = 0\n",
    "    nn_output_train = []\n",
    "    nn_output_test = []\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        XX_train = torch.tensor(X_train).float()\n",
    "        XX_test = torch.tensor(X_test).float()    \n",
    "        #target = torch.tensor(target_tensor).long() \n",
    "\n",
    "        #yy = y_train\n",
    "        output_train = neural_network.forward(XX_train)\n",
    "        output_test = neural_network.forward(XX_test)\n",
    "\n",
    "        for idx, i in enumerate(output_train):\n",
    "    #         print(int(torch.argmax(i)))\n",
    "    #         print(type(torch.argmax(i)))\n",
    "    #        print(type(output_train))\n",
    "            nn_output_train.append(int(torch.argmax(i)))\n",
    "            if torch.argmax(i) == y_train[idx]:\n",
    "                correct_train += 1 ## check the predicted value with the actual value\n",
    "            total_train += 1\n",
    "\n",
    "        for idx, i in enumerate(output_test):\n",
    "            nn_output_test.append(int(torch.argmax(i)))\n",
    "            if torch.argmax(i) == y_test[idx]:\n",
    "                correct_test += 1 ## check the predicted value with the actual value\n",
    "            total_test += 1\n",
    "\n",
    "\n",
    "    y_scores_train = np.array(nn_output_train)\n",
    "    y_scores_test = np.array(nn_output_test)\n",
    "\n",
    "\n",
    "    # print('the accuracy on the training data is is', round(correct_train/total_train, 4))\n",
    "    # print(confusion_matrix(nn_output_train, y13))\n",
    "    # print('the accuracy on the testing data is is', round(correct_test/total_test, 4))\n",
    "    # print(confusion_matrix(nn_output_test, target2))\n",
    "\n",
    "    confusion_train = confusion_matrix(y13, y_scores_train)\n",
    "    confusion_test = confusion_matrix(target2, y_scores_test)\n",
    "\n",
    "\n",
    "    print('Trained')\n",
    "    train_accuracy = (confusion_train[0][0]+confusion_train[1][1])/len(y_train)\n",
    "    train_recall = confusion_train[1][1]/(confusion_train[1][0]+confusion_train[1][1])\n",
    "    train_precision = confusion_train[1][1]/(confusion_train[0][1]+confusion_train[1][1])\n",
    "                                             \n",
    "    test_accuracy = (confusion_test[0][0]+confusion_test[1][1])/len(y_test)\n",
    "    test_recall = confusion_test[1][1]/(confusion_test[1][0]+confusion_test[1][1])\n",
    "    test_precision = confusion_test[1][1]/(confusion_test[0][1]+confusion_test[1][1])\n",
    "                                                                                    \n",
    "    print('Train Accuracy: {}'.format(train_accuracy))\n",
    "    print('Train Recall: {}'.format(train_recall))\n",
    "    print('Train Precision: {}'.format(train_precision))\n",
    "    print(confusion_train)\n",
    "    \n",
    "    print('')\n",
    "    print('Test Accuracy: {}'.format(test_accuracy))\n",
    "    print('Test Recall: {}'.format(test_recall))\n",
    "    print('Test Precision: {}'.format(test_precision))\n",
    "    print(confusion_test)\n",
    "    \n",
    "    return_dic = {'Train_Accuracy': train_accuracy, 'Train_Recall': train_recall, 'Train_Precision': train_precision, \n",
    "                 'Test_Accuracy': test_accuracy, 'Test_Recall': test_recall, 'Test_Precision': test_precision}\n",
    "                                           \n",
    "                                           \n",
    "    return confusion_train, confusion_test, return_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"in order to make a neural network, we need to create an object belonging to the NEURAL NETWORK class\"\"\"\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, name=None):\n",
    "#         \"\"\"inherit NN methods, attributes, functions, etc..., without this we can't use it\"\"\"\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=1*X_train.shape[1], out_features=128) ## (each input is a 2x1 vector) \n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=128) ## (layer1 output, layer 2 output)        \n",
    "        self.fc3 = nn.Linear(in_features=128, out_features=2) ## (layer2 output, layer 3 output)\n",
    "        #self.fc4 = nn.Linear(in_features=64, out_features=2) ## (layer 3 output, layer 4 output)\n",
    "        self.name = None\n",
    "        self.train_size = len(y_train)\n",
    "        self.test_size = len(y_test)\n",
    "        print('the nnn name is {}'.format(self.name))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "#         x = F.relu(self.fc1(x)) ## pass vector into first layer\n",
    "#         x = F.relu(self.fc2(x)) ## 2nd layer\n",
    "#         x = F.relu(self.fc3(x)) ### 3rd layer\n",
    "        #x = F.relu\n",
    "        x = F.leaky_relu(self.fc1(x), inplace=True, negative_slope=0.9)\n",
    "        x = F.leaky_relu(self.fc2(x), inplace=True, negative_slope=0.9)\n",
    "        x = F.leaky_relu(self.fc3(x), inplace=True, negative_slope=0.9)\n",
    "        #x = self.fc4(x) ### and then run softmax regression on the final layer to estimate 0,...,9 \n",
    "        y = F.softmax(x, dim=1)\n",
    "        #y = F.log_softmax(x, dim=1)\n",
    "        return  y\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the nnn name is None\n",
      "epoch 0 had loss 0.6887909770011902\n",
      "final loss was 0.6041503548622131\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [14667, 14666]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-435-5b1507f5f9b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;34m\"\"\"Train 12\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnn12\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_neural_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'NN12_0'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_tensor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCV12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mc12_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc12_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc12_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneural_network\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnn12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCV12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraincv3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;34m\"\"\"Train 13\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-432-4afb052c3291>\u001b[0m in \u001b[0;36mtest_scores\u001b[1;34m(neural_network, X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;31m# print(confusion_matrix(nn_output_test, target2))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[0mconfusion_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my13\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_scores_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m     \u001b[0mconfusion_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_scores_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\PROGRA1\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m     \"\"\"\n\u001b[1;32m--> 253\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not supported\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\PROGRA1\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \"\"\"\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\PROGRA1\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 205\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [14667, 14666]"
     ]
    }
   ],
   "source": [
    "\"\"\"Train 12\"\"\"\n",
    "nn12 = train_neural_network(nn_name='NN12_0', learning_rate=0.001, epochs=100, train_tensor=CV12, target_tensor=y12)\n",
    "c12_train, c12_test, c12_scores = test_scores(neural_network=nn12, X_train=CV12, y_train=y12, X_test=traincv3, y_test=target3)\n",
    "\n",
    "\"\"\"Train 13\"\"\"\n",
    "nn13 = train_neural_network(nn_name='NN13_0', learning_rate=0.001, epochs=100, train_tensor=CV13, target_tensor=y13)\n",
    "c13_train, c13_test, c13_scores = test_scores(neural_network=nn13, X_train=CV13, y_train=y13, X_test=traincv2, y_test=target2)\n",
    "\n",
    "\"\"\"Train 23\"\"\"\n",
    "nn23 = train_neural_network(nn_name='NN23_0', learning_rate=0.001, epochs=100, train_tensor=CV23, target_tensor=y23)\n",
    "c23_train, c23_test, c23_scores = test_scores(neural_network=nn23, X_train=CV23, y_train=y23, X_test=traincv1, y_test=target1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train the Best NN\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20290"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"COPY OF NN\"\"\"\n",
    "\n",
    "# \"\"\"declare neural networks\"\"\"\n",
    "# neural_network12 = Net(name='net12')\n",
    "\n",
    "# def train_12():    \n",
    "#     optimizer12 = optim.Adam(neural_network12.parameters(), lr=0.001) ## Adam optimizer with parameters and learning rate\n",
    "\n",
    "#     EPOCHS = 1200 \n",
    "#     for epoch in range(EPOCHS):\n",
    "#         XX = CV12\n",
    "#         neural_network12.zero_grad()\n",
    "#         output = neural_network12.forward(XX)\n",
    "#         predicted = output.float()\n",
    "#         #yy = y_train\n",
    "#         target = y12\n",
    "#         #print(predicted.shape, target.shape)\n",
    "#         loss = F.cross_entropy(predicted, target)\n",
    "#         #loss = F.nll_loss(output, y) ## calculate loss function with MSE or cross_entropy\n",
    "#         #F.cross_entropy\n",
    "#         #  F.leaky_relu\n",
    "#         loss.backward() ## do the backprop\n",
    "#         optimizer12.step() ## update the weights\n",
    "#         if epoch % 200 == 0:\n",
    "#             print('epoch {} had loss {}'.format(epoch, loss))\n",
    "\n",
    "#     print('final loss was {}\\n\\n'.format(loss))\n",
    "\n",
    "\n",
    "#     ########\n",
    "#     ###########\n",
    "#     #########------Test NN-------------####\n",
    "#     ########\n",
    "#     ##############\n",
    "#     #########\n",
    "\n",
    "#     \"\"\"Now Test on 3rd Dataset\"\"\"\n",
    "#     \"\"\"aaa\"\"\"\n",
    "\n",
    "#     total_train = 0\n",
    "#     correct_train = 0\n",
    "#     total_test = 0\n",
    "#     correct_test = 0\n",
    "\n",
    "#     nn_output_train = []\n",
    "#     nn_output_test = []\n",
    "\n",
    "#     with torch.no_grad(): \n",
    "#         XX_train = CV12\n",
    "#         XX_test = traincv3    \n",
    "\n",
    "#         #yy = y_train\n",
    "#         output_train = neural_network12.forward(XX_train)\n",
    "#         output_test = neural_network12.forward(XX_test)\n",
    "\n",
    "#         for idx, i in enumerate(output_train):\n",
    "#     #         print(int(torch.argmax(i)))\n",
    "#     #         print(type(torch.argmax(i)))\n",
    "#     #        print(type(output_train))\n",
    "#             nn_output_train.append(int(torch.argmax(i)))\n",
    "#             if torch.argmax(i) == y12[idx]:\n",
    "#                 correct_train += 1 ## check the predicted value with the actual value\n",
    "#             total_train += 1\n",
    "\n",
    "#         for idx, i in enumerate(output_test):\n",
    "#             nn_output_test.append(int(torch.argmax(i)))\n",
    "#             if torch.argmax(i) == target3[idx]:\n",
    "#                 correct_test += 1 ## check the predicted value with the actual value\n",
    "#             total_test += 1\n",
    "\n",
    "\n",
    "#     y_scores_train = np.array(nn_output_train)\n",
    "#     y_scores_test = np.array(nn_output_test)\n",
    "\n",
    "#     # print('the accuracy on the training data is is', round(correct_train/total_train, 4))\n",
    "#     # print(confusion_matrix(nn_output_train, y12))\n",
    "#     # print('the accuracy on the testing data is is', round(correct_test/total_test, 4))\n",
    "#     # print(confusion_matrix(nn_output_test, target3))\n",
    "#     confusion_train = confusion_matrix(y12, y_scores_train)\n",
    "#     confusion_test = confusion_matrix(target3, y_scores_test)\n",
    "\n",
    "\n",
    "#     print('Trained on 1,2')\n",
    "#     train_accuracy = confusion_train[0][0]+confusion_train[1][1])/len(y23)\n",
    "#     train_recall = confusion_train[1][1]/(confusion_train[1][0]+confusion_train[1][1]\n",
    "#     train_precision = confusion_train[1][1]/(confusion_train[0][1]+confusion_train[1][1]\n",
    "                                             \n",
    "#     test_accuracy = confusion_test[0][0]+confusion_test[1][1])/len(target1)\n",
    "#     test_recall = confusion_test[1][1]/(confusion_test[1][0]+confusion_test[1][1]\n",
    "#     test_precision = confusion_test[1][1]/(confusion_test[0][1]+confusion_test[1][1]\n",
    "                                                                                    \n",
    "#     print('Train Accuracy: {}'.format(train_accuracy))\n",
    "#     print('Train Recall: {}'.format(train_recall))\n",
    "#     print('Train Precision: {}'.format(train_precision))\n",
    "#     print(confusion_train)\n",
    "    \n",
    "#     print('')\n",
    "#     print('Test Accuracy: {}'.format(test_accuracy))\n",
    "#     print('Test Recall: {}'.format(test_recall))\n",
    "#     print('Test Precision: {}'.format(test_precision))\n",
    "#     print(confusion_test)\n",
    "    \n",
    "#     return_dic = {'Train_Accuracy': train_accuracy, 'Train_Recall': train_recall, 'Train_Precision': train_precision, \n",
    "#                  'Test_Accuracy': test_accuracy, 'Test_Recall': test_recall, 'Test_Precision': test_precision}\n",
    "                                           \n",
    "                                           \n",
    "#     return neural_network12, confusion_train, confusion_test, return_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Train on 1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 had loss 0.6853665113449097\n",
      "epoch 200 had loss 0.6028300523757935\n",
      "epoch 400 had loss 0.6005607843399048\n",
      "epoch 600 had loss 0.5971062183380127\n",
      "epoch 800 had loss 0.595834493637085\n",
      "epoch 1000 had loss 0.5890336036682129\n",
      "final loss was 0.586561381816864\n",
      "\n",
      "\n",
      "Trained on 1,2\n",
      "Train Confusion Matrix with accuracy 0.7130778671757807\n",
      "Train Recall: 0.7573373298363931\n",
      "[[4394 2265]\n",
      " [1943 6064]]\n",
      "\n",
      "Test Confusion Matrix with accuracy 0.6888464685028634\n",
      "Test Recall: 0.7390433258201853\n",
      "[[2101 1240]\n",
      " [1042 2951]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"declare neural networks\"\"\"\n",
    "neural_network12 = Net(name='net12')\n",
    "\n",
    "def train_12():    \n",
    "    optimizer12 = optim.Adam(neural_network12.parameters(), lr=0.001) ## Adam optimizer with parameters and learning rate\n",
    "\n",
    "    EPOCHS = 1200 \n",
    "    for epoch in range(EPOCHS):\n",
    "        XX = CV12\n",
    "        neural_network12.zero_grad()\n",
    "        output = neural_network12.forward(XX)\n",
    "        predicted = output.float()\n",
    "        #yy = y_train\n",
    "        target = y12\n",
    "        #print(predicted.shape, target.shape)\n",
    "        loss = F.cross_entropy(predicted, target)\n",
    "        #loss = F.nll_loss(output, y) ## calculate loss function with MSE or cross_entropy\n",
    "        #F.cross_entropy\n",
    "        #  F.leaky_relu\n",
    "        loss.backward() ## do the backprop\n",
    "        optimizer12.step() ## update the weights\n",
    "        if epoch % 200 == 0:\n",
    "            print('epoch {} had loss {}'.format(epoch, loss))\n",
    "\n",
    "    print('final loss was {}\\n\\n'.format(loss))\n",
    "\n",
    "\n",
    "    ########\n",
    "    ###########\n",
    "    #########------Test NN-------------####\n",
    "    ########\n",
    "    ##############\n",
    "    #########\n",
    "\n",
    "    \"\"\"Now Test on 3rd Dataset\"\"\"\n",
    "    \"\"\"aaa\"\"\"\n",
    "\n",
    "    total_train = 0\n",
    "    correct_train = 0\n",
    "    total_test = 0\n",
    "    correct_test = 0\n",
    "\n",
    "    nn_output_train = []\n",
    "    nn_output_test = []\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        XX_train = CV12\n",
    "        XX_test = traincv3    \n",
    "\n",
    "        #yy = y_train\n",
    "        output_train = neural_network12.forward(XX_train)\n",
    "        output_test = neural_network12.forward(XX_test)\n",
    "\n",
    "        for idx, i in enumerate(output_train):\n",
    "    #         print(int(torch.argmax(i)))\n",
    "    #         print(type(torch.argmax(i)))\n",
    "    #        print(type(output_train))\n",
    "            nn_output_train.append(int(torch.argmax(i)))\n",
    "            if torch.argmax(i) == y12[idx]:\n",
    "                correct_train += 1 ## check the predicted value with the actual value\n",
    "            total_train += 1\n",
    "\n",
    "        for idx, i in enumerate(output_test):\n",
    "            nn_output_test.append(int(torch.argmax(i)))\n",
    "            if torch.argmax(i) == target3[idx]:\n",
    "                correct_test += 1 ## check the predicted value with the actual value\n",
    "            total_test += 1\n",
    "\n",
    "\n",
    "    y_scores_train = np.array(nn_output_train)\n",
    "    y_scores_test = np.array(nn_output_test)\n",
    "\n",
    "    # print('the accuracy on the training data is is', round(correct_train/total_train, 4))\n",
    "    # print(confusion_matrix(nn_output_train, y12))\n",
    "    # print('the accuracy on the testing data is is', round(correct_test/total_test, 4))\n",
    "    # print(confusion_matrix(nn_output_test, target3))\n",
    "    confusion_train = confusion_matrix(y12, y_scores_train)\n",
    "    confusion_test = confusion_matrix(target3, y_scores_test)\n",
    "\n",
    "\n",
    "    print('Trained on 1,2')\n",
    "    train_accuracy = confusion_train[0][0]+confusion_train[1][1])/len(y23)\n",
    "    train_recall = confusion_train[1][1]/(confusion_train[1][0]+confusion_train[1][1]\n",
    "    train_precision = confusion_train[1][1]/(confusion_train[0][1]+confusion_train[1][1]\n",
    "                                             \n",
    "    test_accuracy = confusion_test[0][0]+confusion_test[1][1])/len(target1)\n",
    "    test_recall = confusion_test[1][1]/(confusion_test[1][0]+confusion_test[1][1]\n",
    "    test_precision = confusion_test[1][1]/(confusion_test[0][1]+confusion_test[1][1]\n",
    "                                                                                    \n",
    "    print('Train Accuracy: {}'.format(train_accuracy))\n",
    "    print('Train Recall: {}'.format(train_recall))\n",
    "    print('Train Precision: {}'.format(train_precision))\n",
    "    print(confusion_train)\n",
    "    \n",
    "    print('')\n",
    "    print('Test Accuracy: {}'.format(test_accuracy))\n",
    "    print('Test Recall: {}'.format(test_recall))\n",
    "    print('Test Precision: {}'.format(test_precision))\n",
    "    print(confusion_test)\n",
    "    \n",
    "    return_dic = {'Train_Accuracy': train_accuracy, 'Train_Recall': train_recall, 'Train_Precision': train_precision, \n",
    "                 'Test_Accuracy': test_accuracy, 'Test_Recall': test_recall, 'Test_Precision': test_precision}\n",
    "                                           \n",
    "                                           \n",
    "    return neural_network12, confusion_train, confusion_test, return_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Train on 1,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(nn_name, learning_rate, epochs, train_tensor, target_tensor): \n",
    "    \"\"\"declare name\"\"\"\n",
    "    neural_network = Net(name=nn_name)\n",
    "    optimizer = optim.Adam(neural_network.parameters(), lr=learning_rate) ## Adam optimizer with parameters and learning rate\n",
    "    EPOCHS = epochs\n",
    "    \n",
    "    XX = torch.tensor(train_tensor).float()\n",
    "    target = torch.tensor(target_tensor).long()\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        XX = XX\n",
    "        target = target\n",
    "        \n",
    "        neural_network.zero_grad()\n",
    "        output = neural_network.forward(XX)\n",
    "        predicted = output.float()\n",
    "        \n",
    "        #print(predicted.shape, target.shape)\n",
    "        loss = F.cross_entropy(predicted, target)\n",
    "        #loss = F.nll_loss(output, y) ## calculate loss function with MSE or cross_entropy\n",
    "        #F.cross_entropy\n",
    "        #  F.leaky_relu\n",
    "        loss.backward() ## do the backprop\n",
    "        optimizer.step() ## update the weights\n",
    "        if epoch % 200 == 0:\n",
    "            print('epoch {} had loss {}'.format(epoch, loss))\n",
    "\n",
    "    print('final loss was {}\\n\\n'.format(loss))\n",
    "    \n",
    "    return neural_network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the nnn name is None\n",
      "epoch 0 had loss 0.6935456991195679\n",
      "final loss was 0.6042400598526001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samp_nn = train_neural_network('franlin',0.001, 100, CV13, y13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_scores(neural_network, X_train, y_train, X_test, y_test):\n",
    "    total_train = 0\n",
    "    correct_train = 0\n",
    "    total_test = 0\n",
    "    correct_test = 0\n",
    "    nn_output_train = []\n",
    "    nn_output_test = []\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        XX_train = torch.tensor(X_train).float()\n",
    "        XX_test = torch.tensor(X_test).float()    \n",
    "        #target = torch.tensor(target_tensor).long() \n",
    "\n",
    "        #yy = y_train\n",
    "        output_train = neural_network.forward(XX_train)\n",
    "        output_test = neural_network.forward(XX_test)\n",
    "\n",
    "        for idx, i in enumerate(output_train):\n",
    "    #         print(int(torch.argmax(i)))\n",
    "    #         print(type(torch.argmax(i)))\n",
    "    #        print(type(output_train))\n",
    "            nn_output_train.append(int(torch.argmax(i)))\n",
    "            if torch.argmax(i) == y_train[idx]:\n",
    "                correct_train += 1 ## check the predicted value with the actual value\n",
    "            total_train += 1\n",
    "\n",
    "        for idx, i in enumerate(output_test):\n",
    "            nn_output_test.append(int(torch.argmax(i)))\n",
    "            if torch.argmax(i) == y_test[idx]:\n",
    "                correct_test += 1 ## check the predicted value with the actual value\n",
    "            total_test += 1\n",
    "\n",
    "\n",
    "    y_scores_train = np.array(nn_output_train)\n",
    "    y_scores_test = np.array(nn_output_test)\n",
    "\n",
    "\n",
    "    # print('the accuracy on the training data is is', round(correct_train/total_train, 4))\n",
    "    # print(confusion_matrix(nn_output_train, y13))\n",
    "    # print('the accuracy on the testing data is is', round(correct_test/total_test, 4))\n",
    "    # print(confusion_matrix(nn_output_test, target2))\n",
    "\n",
    "    confusion_train = confusion_matrix(y13, y_scores_train)\n",
    "    confusion_test = confusion_matrix(target2, y_scores_test)\n",
    "\n",
    "\n",
    "    print('Trained')\n",
    "    train_accuracy = (confusion_train[0][0]+confusion_train[1][1])/len(y_train)\n",
    "    train_recall = confusion_train[1][1]/(confusion_train[1][0]+confusion_train[1][1])\n",
    "    train_precision = confusion_train[1][1]/(confusion_train[0][1]+confusion_train[1][1])\n",
    "                                             \n",
    "    test_accuracy = (confusion_test[0][0]+confusion_test[1][1])/len(y_test)\n",
    "    test_recall = confusion_test[1][1]/(confusion_test[1][0]+confusion_test[1][1])\n",
    "    test_precision = confusion_test[1][1]/(confusion_test[0][1]+confusion_test[1][1])\n",
    "                                                                                    \n",
    "    print('Train Accuracy: {}'.format(train_accuracy))\n",
    "    print('Train Recall: {}'.format(train_recall))\n",
    "    print('Train Precision: {}'.format(train_precision))\n",
    "    print(confusion_train)\n",
    "    \n",
    "    print('')\n",
    "    print('Test Accuracy: {}'.format(test_accuracy))\n",
    "    print('Test Recall: {}'.format(test_recall))\n",
    "    print('Test Precision: {}'.format(test_precision))\n",
    "    print(confusion_test)\n",
    "    \n",
    "    return_dic = {'Train_Accuracy': train_accuracy, 'Train_Recall': train_recall, 'Train_Precision': train_precision, \n",
    "                 'Test_Accuracy': test_accuracy, 'Test_Recall': test_recall, 'Test_Precision': test_precision}\n",
    "                                           \n",
    "                                           \n",
    "    return neural_network, confusion_train, confusion_test, return_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # \"\"\"declare name\"\"\"\n",
    "# # neural_network13 = Net(name='net13')\n",
    "\n",
    "# def train_13():\n",
    "#     \"\"\"declare name\"\"\"\n",
    "#     neural_network = Net(name='net13')\n",
    "#     print('Neural network name is {}'.format(neural_network.name))\n",
    "#     optimizer = optim.Adam(neural_network.parameters(), lr=0.001) ## Adam optimizer with parameters and learning rate\n",
    "#     EPOCHS = 1200\n",
    "    \n",
    "    \n",
    "#     for epoch in range(EPOCHS):\n",
    "#         XX = CV13\n",
    "#         neural_network.zero_grad()\n",
    "#         output = neural_network.forward(XX)\n",
    "#         predicted = output.float()\n",
    "#         #yy = y_train\n",
    "#         target = y13\n",
    "#         #print(predicted.shape, target.shape)\n",
    "#         loss = F.cross_entropy(predicted, target)\n",
    "#         #loss = F.nll_loss(output, y) ## calculate loss function with MSE or cross_entropy\n",
    "#         #F.cross_entropy\n",
    "#         #  F.leaky_relu\n",
    "#         loss.backward() ## do the backprop\n",
    "#         optimizer.step() ## update the weights\n",
    "#         if epoch % 200 == 0:\n",
    "#             print('epoch {} had loss {}'.format(epoch, loss))\n",
    "\n",
    "#     print('final loss was {}\\n\\n'.format(loss))\n",
    "\n",
    "\n",
    "#     ########\n",
    "#     ###########\n",
    "#     #########------Test NN-------------####\n",
    "#     ########\n",
    "#     ##############\n",
    "#     #########\n",
    "\n",
    "\n",
    "#     \"\"\"Now Test on 2nd Dataset\"\"\"\n",
    "#     \"\"\"aaa\"\"\"\n",
    "\n",
    "#     total_train = 0\n",
    "#     correct_train = 0\n",
    "#     total_test = 0\n",
    "#     correct_test = 0\n",
    "\n",
    "#     nn_output_train = []\n",
    "#     nn_output_test = []\n",
    "\n",
    "#     with torch.no_grad(): \n",
    "#         XX_train = CV13\n",
    "#         XX_test = traincv2    \n",
    "\n",
    "#         #yy = y_train\n",
    "#         output_train = neural_network13.forward(XX_train)\n",
    "#         output_test = neural_network13.forward(XX_test)\n",
    "\n",
    "#         for idx, i in enumerate(output_train):\n",
    "#     #         print(int(torch.argmax(i)))\n",
    "#     #         print(type(torch.argmax(i)))\n",
    "#     #        print(type(output_train))\n",
    "#             nn_output_train.append(int(torch.argmax(i)))\n",
    "#             if torch.argmax(i) == y13[idx]:\n",
    "#                 correct_train += 1 ## check the predicted value with the actual value\n",
    "#             total_train += 1\n",
    "\n",
    "#         for idx, i in enumerate(output_test):\n",
    "#             nn_output_test.append(int(torch.argmax(i)))\n",
    "#             if torch.argmax(i) == target2[idx]:\n",
    "#                 correct_test += 1 ## check the predicted value with the actual value\n",
    "#             total_test += 1\n",
    "\n",
    "\n",
    "#     y_scores_train = np.array(nn_output_train)\n",
    "#     y_scores_test = np.array(nn_output_test)\n",
    "\n",
    "\n",
    "#     # print('the accuracy on the training data is is', round(correct_train/total_train, 4))\n",
    "#     # print(confusion_matrix(nn_output_train, y13))\n",
    "#     # print('the accuracy on the testing data is is', round(correct_test/total_test, 4))\n",
    "#     # print(confusion_matrix(nn_output_test, target2))\n",
    "\n",
    "#     confusion_train = confusion_matrix(y13, y_scores_train)\n",
    "#     confusion_test = confusion_matrix(target2, y_scores_test)\n",
    "\n",
    "\n",
    "#     print('Trained on 1,3')\n",
    "#     train_accuracy = confusion_train[0][0]+confusion_train[1][1])/len(y23)\n",
    "#     train_recall = confusion_train[1][1]/(confusion_train[1][0]+confusion_train[1][1]\n",
    "#     train_precision = confusion_train[1][1]/(confusion_train[0][1]+confusion_train[1][1]\n",
    "                                             \n",
    "#     test_accuracy = confusion_test[0][0]+confusion_test[1][1])/len(target1)\n",
    "#     test_recall = confusion_test[1][1]/(confusion_test[1][0]+confusion_test[1][1]\n",
    "#     test_precision = confusion_test[1][1]/(confusion_test[0][1]+confusion_test[1][1]\n",
    "                                                                                    \n",
    "#     print('Train Accuracy: {}'.format(train_accuracy))\n",
    "#     print('Train Recall: {}'.format(train_recall))\n",
    "#     print('Train Precision: {}'.format(train_precision))\n",
    "#     print(confusion_train)\n",
    "    \n",
    "#     print('')\n",
    "#     print('Test Accuracy: {}'.format(test_accuracy))\n",
    "#     print('Test Recall: {}'.format(test_recall))\n",
    "#     print('Test Precision: {}'.format(test_precision))\n",
    "#     print(confusion_test)\n",
    "    \n",
    "#     return_dic = {'Train_Accuracy': train_accuracy, 'Train_Recall': train_recall, 'Train_Precision': train_precision, \n",
    "#                  'Test_Accuracy': test_accuracy, 'Test_Recall': test_recall, 'Test_Precision': test_precision}\n",
    "                                           \n",
    "                                           \n",
    "#     return neural_network13, confusion_train, confusion_test, return_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # \"\"\"declare name\"\"\"\n",
    "# # neural_network13 = Net(name='net13')\n",
    "\n",
    "# def train_13():\n",
    "#     \"\"\"declare name\"\"\"\n",
    "#     neural_network = Net(name='net13')\n",
    "#     print('Neural network name is {}'.format(neural_network.name))\n",
    "#     optimizer = optim.Adam(neural_network.parameters(), lr=0.001) ## Adam optimizer with parameters and learning rate\n",
    "#     EPOCHS = 1200\n",
    "    \n",
    "    \n",
    "#     for epoch in range(EPOCHS):\n",
    "#         XX = CV13\n",
    "#         neural_network.zero_grad()\n",
    "#         output = neural_network13.forward(XX)\n",
    "#         predicted = output.float()\n",
    "#         #yy = y_train\n",
    "#         target = y13\n",
    "#         #print(predicted.shape, target.shape)\n",
    "#         loss = F.cross_entropy(predicted, target)\n",
    "#         #loss = F.nll_loss(output, y) ## calculate loss function with MSE or cross_entropy\n",
    "#         #F.cross_entropy\n",
    "#         #  F.leaky_relu\n",
    "#         loss.backward() ## do the backprop\n",
    "#         optimizer13.step() ## update the weights\n",
    "#         if epoch % 200 == 0:\n",
    "#             print('epoch {} had loss {}'.format(epoch, loss))\n",
    "\n",
    "#     print('final loss was {}\\n\\n'.format(loss))\n",
    "\n",
    "\n",
    "#     ########\n",
    "#     ###########\n",
    "#     #########------Test NN-------------####\n",
    "#     ########\n",
    "#     ##############\n",
    "#     #########\n",
    "\n",
    "\n",
    "#     \"\"\"Now Test on 2nd Dataset\"\"\"\n",
    "#     \"\"\"aaa\"\"\"\n",
    "\n",
    "#     total_train = 0\n",
    "#     correct_train = 0\n",
    "#     total_test = 0\n",
    "#     correct_test = 0\n",
    "\n",
    "#     nn_output_train = []\n",
    "#     nn_output_test = []\n",
    "\n",
    "#     with torch.no_grad(): \n",
    "#         XX_train = CV13\n",
    "#         XX_test = traincv2    \n",
    "\n",
    "#         #yy = y_train\n",
    "#         output_train = neural_network13.forward(XX_train)\n",
    "#         output_test = neural_network13.forward(XX_test)\n",
    "\n",
    "#         for idx, i in enumerate(output_train):\n",
    "#     #         print(int(torch.argmax(i)))\n",
    "#     #         print(type(torch.argmax(i)))\n",
    "#     #        print(type(output_train))\n",
    "#             nn_output_train.append(int(torch.argmax(i)))\n",
    "#             if torch.argmax(i) == y13[idx]:\n",
    "#                 correct_train += 1 ## check the predicted value with the actual value\n",
    "#             total_train += 1\n",
    "\n",
    "#         for idx, i in enumerate(output_test):\n",
    "#             nn_output_test.append(int(torch.argmax(i)))\n",
    "#             if torch.argmax(i) == target2[idx]:\n",
    "#                 correct_test += 1 ## check the predicted value with the actual value\n",
    "#             total_test += 1\n",
    "\n",
    "\n",
    "#     y_scores_train = np.array(nn_output_train)\n",
    "#     y_scores_test = np.array(nn_output_test)\n",
    "\n",
    "\n",
    "#     # print('the accuracy on the training data is is', round(correct_train/total_train, 4))\n",
    "#     # print(confusion_matrix(nn_output_train, y13))\n",
    "#     # print('the accuracy on the testing data is is', round(correct_test/total_test, 4))\n",
    "#     # print(confusion_matrix(nn_output_test, target2))\n",
    "\n",
    "#     confusion_train = confusion_matrix(y13, y_scores_train)\n",
    "#     confusion_test = confusion_matrix(target2, y_scores_test)\n",
    "\n",
    "\n",
    "#     print('Trained on 1,3')\n",
    "#     train_accuracy = confusion_train[0][0]+confusion_train[1][1])/len(y23)\n",
    "#     train_recall = confusion_train[1][1]/(confusion_train[1][0]+confusion_train[1][1]\n",
    "#     train_precision = confusion_train[1][1]/(confusion_train[0][1]+confusion_train[1][1]\n",
    "                                             \n",
    "#     test_accuracy = confusion_test[0][0]+confusion_test[1][1])/len(target1)\n",
    "#     test_recall = confusion_test[1][1]/(confusion_test[1][0]+confusion_test[1][1]\n",
    "#     test_precision = confusion_test[1][1]/(confusion_test[0][1]+confusion_test[1][1]\n",
    "                                                                                    \n",
    "#     print('Train Accuracy: {}'.format(train_accuracy))\n",
    "#     print('Train Recall: {}'.format(train_recall))\n",
    "#     print('Train Precision: {}'.format(train_precision))\n",
    "#     print(confusion_train)\n",
    "    \n",
    "#     print('')\n",
    "#     print('Test Accuracy: {}'.format(test_accuracy))\n",
    "#     print('Test Recall: {}'.format(test_recall))\n",
    "#     print('Test Precision: {}'.format(test_precision))\n",
    "#     print(confusion_test)\n",
    "    \n",
    "#     return_dic = {'Train_Accuracy': train_accuracy, 'Train_Recall': train_recall, 'Train_Precision': train_precision, \n",
    "#                  'Test_Accuracy': test_accuracy, 'Test_Recall': test_recall, 'Test_Precision': test_precision}\n",
    "                                           \n",
    "                                           \n",
    "#     return neural_network13, confusion_train, confusion_test, return_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Train 2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"delcare name\"\"\"\n",
    "neural_network23 = Net(name='net23')\n",
    "\n",
    "def train_23():\n",
    "    optimizer23 = optim.Adam(neural_network23.parameters(), lr=0.001) ## Adam optimizer with parameters and learning rate\n",
    "\n",
    "    EPOCHS = 1200 \n",
    "    for epoch in range(EPOCHS):\n",
    "        XX = CV23\n",
    "        neural_network23.zero_grad()\n",
    "        output = neural_network23.forward(XX)\n",
    "        predicted = output.float()\n",
    "        #yy = y_train\n",
    "        target = y23\n",
    "        #print(predicted.shape, target.shape)\n",
    "        loss = F.cross_entropy(predicted, target)\n",
    "        #loss = F.nll_loss(output, y) ## calculate loss function with MSE or cross_entropy\n",
    "        #F.cross_entropy\n",
    "        #  F.leaky_relu\n",
    "        loss.backward() ## do the backprop\n",
    "        optimizer23.step() ## update the weights\n",
    "        if epoch % 200 == 0:\n",
    "            print('epoch {} had loss {}'.format(epoch, loss))\n",
    "\n",
    "    print('final loss was {}\\n\\n'.format(loss))\n",
    "\n",
    "    ########\n",
    "    ###########\n",
    "    #########------Test NN-------------####\n",
    "    ########\n",
    "    ##############\n",
    "    #########\n",
    "    \"\"\"Now Test on 1st Dataset\"\"\"\n",
    "    \"\"\"aaa\"\"\"\n",
    "\n",
    "    total_train = 0\n",
    "    correct_train = 0\n",
    "    total_test = 0\n",
    "    correct_test = 0\n",
    "\n",
    "    nn_output_train = []\n",
    "    nn_output_test = []\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        XX_train = CV23\n",
    "        XX_test = traincv1    \n",
    "\n",
    "        #yy = y_train\n",
    "        output_train = neural_network23.forward(XX_train)\n",
    "        output_test = neural_network23.forward(XX_test)\n",
    "\n",
    "        for idx, i in enumerate(output_train):\n",
    "    #         print(int(torch.argmax(i)))\n",
    "    #         print(type(torch.argmax(i)))\n",
    "    #        print(type(output_train))\n",
    "            nn_output_train.append(int(torch.argmax(i)))\n",
    "            if torch.argmax(i) == y23[idx]:\n",
    "                correct_train += 1 ## check the predicted value with the actual value\n",
    "            total_train += 1\n",
    "\n",
    "        for idx, i in enumerate(output_test):\n",
    "            nn_output_test.append(int(torch.argmax(i)))\n",
    "            if torch.argmax(i) == target1[idx]:\n",
    "                correct_test += 1 ## check the predicted value with the actual value\n",
    "            total_test += 1\n",
    "\n",
    "\n",
    "    y_scores_train = np.array(nn_output_train)\n",
    "    y_scores_test = np.array(nn_output_test)\n",
    "\n",
    "    # print('the accuracy on the training data is is', round(correct_train/total_train, 4))\n",
    "    # print(confusion_matrix(nn_output_train, y23))\n",
    "    # print('the accuracy on the testing data is is', round(correct_test/total_test, 4))\n",
    "    # print(confusion_matrix(nn_output_test, target1))\n",
    "\n",
    "\n",
    "    confusion_train = confusion_matrix(y23, y_scores_train)\n",
    "    confusion_test = confusion_matrix(target1, y_scores_test)\n",
    "\n",
    "\n",
    "    print('Trained on 2,3')\n",
    "    train_accuracy = confusion_train[0][0]+confusion_train[1][1])/len(y23)\n",
    "    train_recall = confusion_train[1][1]/(confusion_train[1][0]+confusion_train[1][1]\n",
    "    train_precision = confusion_train[1][1]/(confusion_train[0][1]+confusion_train[1][1]\n",
    "                                             \n",
    "    test_accuracy = confusion_test[0][0]+confusion_test[1][1])/len(target1)\n",
    "    test_recall = confusion_test[1][1]/(confusion_test[1][0]+confusion_test[1][1]\n",
    "    test_precision = confusion_test[1][1]/(confusion_test[0][1]+confusion_test[1][1]\n",
    "                                                                                    \n",
    "    print('Train Accuracy: {}'.format(train_accuracy))\n",
    "    print('Train Recall: {}'.format(train_recall))\n",
    "    print('Train Precision: {}'.format(train_precision))\n",
    "    print(confusion_train)\n",
    "    \n",
    "    print('')\n",
    "    print('Test Accuracy: {}'.format(test_accuracy))\n",
    "    print('Test Recall: {}'.format(test_recall))\n",
    "    print('Test Precision: {}'.format(test_precision))\n",
    "    print(confusion_test)\n",
    "    \n",
    "    return_dic = {'Train_Accuracy': train_accuracy, 'Train_Recall': train_recall, 'Train_Precision': train_precision, \n",
    "                 'Test_Accuracy': test_accuracy, 'Test_Recall': test_recall, 'Test_Precision': test_precision}\n",
    "                                           \n",
    "                                           \n",
    "    return neural_network23, confusion_train, confusion_test, return_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "##PRACITC\n",
    "# \"\"\"delcare name\"\"\"\n",
    "# neural_network23 = Net(name='net23')\n",
    "\n",
    "# def train_23():\n",
    "#     optimizer23 = optim.Adam(neural_network23.parameters(), lr=0.001) ## Adam optimizer with parameters and learning rate\n",
    "\n",
    "#     EPOCHS = 1200 \n",
    "#     for epoch in range(EPOCHS):\n",
    "#         XX = CV23\n",
    "#         neural_network23.zero_grad()\n",
    "#         output = neural_network23.forward(XX)\n",
    "#         predicted = output.float()\n",
    "#         #yy = y_train\n",
    "#         target = y23\n",
    "#         #print(predicted.shape, target.shape)\n",
    "#         loss = F.cross_entropy(predicted, target)\n",
    "#         #loss = F.nll_loss(output, y) ## calculate loss function with MSE or cross_entropy\n",
    "#         #F.cross_entropy\n",
    "#         #  F.leaky_relu\n",
    "#         loss.backward() ## do the backprop\n",
    "#         optimizer23.step() ## update the weights\n",
    "#         if epoch % 200 == 0:\n",
    "#             print('epoch {} had loss {}'.format(epoch, loss))\n",
    "\n",
    "#     print('final loss was {}\\n\\n'.format(loss))\n",
    "\n",
    "#     ########\n",
    "#     ###########\n",
    "#     #########------Test NN-------------####\n",
    "#     ########\n",
    "#     ##############\n",
    "#     #########\n",
    "#     \"\"\"Now Test on 1st Dataset\"\"\"\n",
    "#     \"\"\"aaa\"\"\"\n",
    "\n",
    "#     total_train = 0\n",
    "#     correct_train = 0\n",
    "#     total_test = 0\n",
    "#     correct_test = 0\n",
    "\n",
    "#     nn_output_train = []\n",
    "#     nn_output_test = []\n",
    "\n",
    "#     with torch.no_grad(): \n",
    "#         XX_train = CV23\n",
    "#         XX_test = traincv1    \n",
    "\n",
    "#         #yy = y_train\n",
    "#         output_train = neural_network23.forward(XX_train)\n",
    "#         output_test = neural_network23.forward(XX_test)\n",
    "\n",
    "#         for idx, i in enumerate(output_train):\n",
    "#     #         print(int(torch.argmax(i)))\n",
    "#     #         print(type(torch.argmax(i)))\n",
    "#     #        print(type(output_train))\n",
    "#             nn_output_train.append(int(torch.argmax(i)))\n",
    "#             if torch.argmax(i) == y23[idx]:\n",
    "#                 correct_train += 1 ## check the predicted value with the actual value\n",
    "#             total_train += 1\n",
    "\n",
    "#         for idx, i in enumerate(output_test):\n",
    "#             nn_output_test.append(int(torch.argmax(i)))\n",
    "#             if torch.argmax(i) == target1[idx]:\n",
    "#                 correct_test += 1 ## check the predicted value with the actual value\n",
    "#             total_test += 1\n",
    "\n",
    "\n",
    "#     y_scores_train = np.array(nn_output_train)\n",
    "#     y_scores_test = np.array(nn_output_test)\n",
    "\n",
    "#     # print('the accuracy on the training data is is', round(correct_train/total_train, 4))\n",
    "#     # print(confusion_matrix(nn_output_train, y23))\n",
    "#     # print('the accuracy on the testing data is is', round(correct_test/total_test, 4))\n",
    "#     # print(confusion_matrix(nn_output_test, target1))\n",
    "\n",
    "\n",
    "#     confusion_train = confusion_matrix(y23, y_scores_train)\n",
    "#     confusion_test = confusion_matrix(target1, y_scores_test)\n",
    "\n",
    "\n",
    "#     print('Trained on 2,3')\n",
    "#     train_accuracy = confusion_train[0][0]+confusion_train[1][1])/len(y23)\n",
    "#     train_recall = confusion_train[1][1]/(confusion_train[1][0]+confusion_train[1][1]\n",
    "#     train_precision = confusion_train[1][1]/(confusion_train[0][1]+confusion_train[1][1]\n",
    "                                             \n",
    "#     test_accuracy = confusion_test[0][0]+confusion_test[1][1])/len(target1)\n",
    "#     test_recall = confusion_test[1][1]/(confusion_test[1][0]+confusion_test[1][1]\n",
    "#     test_precision = confusion_test[1][1]/(confusion_test[0][1]+confusion_test[1][1]\n",
    "                                                                                    \n",
    "#     print('Train Accuracy: {}'.format(train_accuracy))\n",
    "#     print('Train Recall: {}'.format(train_recall))\n",
    "#     print('Train Precision: {}'.format(train_precision))\n",
    "#     print(confusion_train)\n",
    "    \n",
    "#     print('')\n",
    "#     print('Test Accuracy: {}'.format(test_accuracy))\n",
    "#     print('Test Recall: {}'.format(test_recall))\n",
    "#     print('Test Precision: {}'.format(test_precision))\n",
    "#     print(confusion_test)\n",
    "    \n",
    "#     return_dic = {'Train_Accuracy': train_accuracy, 'Train_Recall': train_recall, 'Train_Precision': train_precision, \n",
    "#                  'Test_Accuracy': test_accuracy, 'Test_Recall': test_recall, 'Test_Precision': test_precision}\n",
    "                                           \n",
    "                                           \n",
    "#     return neural_network23, confusion_train, confusion_test, return_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain with Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_models_df = pd.DataFrame(columns=['NN', 'HL1', 'HL2', 'HL3', 'HL4', 'HL5', 'TN', 'FP', 'FN', 'TP'])\n",
    "sample_nn = {'NN': 0, 'LR': 0, 'Layers': 0, 'Total_Layers': 0,\n",
    "             'Train_TN': 0, 'Train_TP':0, 'Train_FP':0, 'Train_FN':0, \n",
    "             'Test_TN': 0, 'Test_TP':0, 'Test_FP':0, 'Test_FN':0}\n",
    "\n",
    "sample_nn = pd.DataFrame([sample_nn])\n",
    "\n",
    "best_models_df = sample_nn.copy()\n",
    "best_models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 had loss 0.5885718464851379\n",
      "epoch 200 had loss 0.5964018702507019\n",
      "epoch 400 had loss 0.5847443342208862\n",
      "epoch 600 had loss 0.5839317440986633\n",
      "epoch 800 had loss 0.5869135856628418\n",
      "epoch 1000 had loss 0.5872578024864197\n",
      "final loss was 0.5808111429214478\n",
      "\n",
      "\n",
      "Trained on FULL Training Data\n",
      "Train Confusion Matrix with accuracy 0.7222272727272727\n",
      "Train Recall: 0.746\n",
      "[[6937 3063]\n",
      " [3048 8952]]\n",
      "\n",
      "Test Confusion Matrix with accuracy 0.6847708230655495\n",
      "Test Recall: 0.7294\n",
      "[[6600 3690]\n",
      " [2706 7294]]\n"
     ]
    }
   ],
   "source": [
    "neural_network_best = neural_network23\n",
    "\"\"\"NN, \"\"\"\n",
    "nn_name = neural_network_best.name\n",
    "learning_rate = 0.001\n",
    "n_layers = int(str(neural_network_best).split('fc')[-1].split(')')[0])\n",
    "\n",
    "\n",
    "optimizer_best = optim.Adam(neural_network_best.parameters(), lr=learning_rate) ## Adam optimizer with parameters and learning rate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EPOCHS = 1200 \n",
    "for epoch in range(EPOCHS):\n",
    "    XX = train\n",
    "    neural_network_best.zero_grad()\n",
    "    output = neural_network_best.forward(XX)\n",
    "    predicted = output.float()\n",
    "    #yy = y_train\n",
    "    target = y_train\n",
    "    #print(predicted.shape, target.shape)\n",
    "    loss = F.cross_entropy(predicted, target)\n",
    "    #loss = F.nll_loss(output, y) ## calculate loss function with MSE or cross_entropy\n",
    "    #F.cross_entropy\n",
    "    #  F.leaky_relu\n",
    "    loss.backward() ## do the backprop\n",
    "    optimizer_best.step() ## update the weights\n",
    "    if epoch % 200 == 0:\n",
    "        print('epoch {} had loss {}'.format(epoch, loss))\n",
    "\n",
    "print('final loss was {}\\n\\n'.format(loss))\n",
    "\n",
    "\n",
    "########\n",
    "###########\n",
    "#########------Test NN-------------####\n",
    "########\n",
    "##############\n",
    "#########\n",
    "\n",
    "\n",
    "\"\"\"Now Test on FULL Training Dataset\"\"\"\n",
    "\"\"\"aaa\"\"\"\n",
    "\n",
    "total_train = 0\n",
    "correct_train = 0\n",
    "total_test = 0\n",
    "correct_test = 0\n",
    "\n",
    "nn_output_train = []\n",
    "nn_output_test = []\n",
    "\n",
    "with torch.no_grad(): \n",
    "    XX_train = train\n",
    "    XX_test = test    \n",
    "    \n",
    "    #input the matrix and output the tensor of classifications\n",
    "    output_train = neural_network_best.forward(XX_train)\n",
    "    output_test = neural_network_best.forward(XX_test)\n",
    "    \n",
    "    for idx, i in enumerate(output_train):\n",
    "#         print(int(torch.argmax(i)))\n",
    "#         print(type(torch.argmax(i)))\n",
    "#        print(type(output_train))\n",
    "        nn_output_train.append(int(torch.argmax(i)))\n",
    "        if torch.argmax(i) == y_train[idx]:\n",
    "            correct_train += 1 ## check the predicted value with the actual value\n",
    "        total_train += 1\n",
    "\n",
    "    for idx, i in enumerate(output_test):\n",
    "        nn_output_test.append(int(torch.argmax(i)))\n",
    "        if torch.argmax(i) == y_test[idx]:\n",
    "            correct_test += 1 ## check the predicted value with the actual value\n",
    "        total_test += 1\n",
    "            \n",
    "\n",
    "y_scores_train = np.array(nn_output_train)\n",
    "y_scores_test = np.array(nn_output_test)\n",
    "\n",
    "\n",
    "# print('the accuracy on the training data is is', round(correct_train/total_train, 4))\n",
    "# print(confusion_matrix(nn_output_train, y13))\n",
    "# print('the accuracy on the testing data is is', round(correct_test/total_test, 4))\n",
    "# print(confusion_matrix(nn_output_test, target2))\n",
    "\n",
    "confusion_train = confusion_matrix(y_train, y_scores_train)\n",
    "confusion_test = confusion_matrix(y_test, y_scores_test)\n",
    "\n",
    "\n",
    "print('Trained on FULL Training Data')\n",
    "print('Train Confusion Matrix with accuracy {}'.format((confusion_train[0][0]+confusion_train[1][1])/len(y_train)))\n",
    "print('Train Recall: {}'.format((confusion_train[1][1]/(confusion_train[1][0]+confusion_train[1][1]))))\n",
    "print(confusion_train)\n",
    "print('')\n",
    "print('Test Confusion Matrix with accuracy {}'.format((confusion_test[0][0]+confusion_test[1][1])/len(y_test)))\n",
    "print('Test Recall: {}'.format((confusion_test[1][1]/(confusion_test[1][0]+confusion_test[1][1]))))\n",
    "print(confusion_test)\n",
    "\n",
    "\n",
    "#########\n",
    "###########\n",
    "########-----Append the data into a new dictionary----##\n",
    "###########\n",
    "#############\n",
    "#########\n",
    "\"\"\"append dictionary to dataframe\"\"\"\n",
    "\n",
    "nn_data = {'NN': 0, 'LR': 0, 'Layers': 0, 'Total_Layers': 0,\n",
    "             'Train_TN': 0, 'Train_TP':0, 'Train_FP':0, 'Train_FN':0, \n",
    "             'Test_TN': 0, 'Test_TP':0, 'Test_FP':0, 'Test_FN':0}\n",
    "\n",
    "nn_data['NN'] = nn_name\n",
    "nn_data['LR'] = learning_rate\n",
    "nn_data['Total_Layers']= n_layers\n",
    "\n",
    "layer_list = np.array([[]])\n",
    "for hidden_layer_index in range(n_layers): \n",
    "    arc = str(neural_network_best).split('fc')[1:][hidden_layer_index].split(',')\n",
    "    in_layer = int(arc[0].split(': ')[1].split('=')[1])\n",
    "    out_layer = int(arc[1].split('=')[1])\n",
    "    layer_values = np.array([in_layer, out_layer])\n",
    "    layer_list = np.append(arr=[layer_list], values=[layer_values])\n",
    "\n",
    "nn_data['Layers'] = layer_list\n",
    "\n",
    "nn_data['Train_TN'] = confusion_train[0][0]\n",
    "nn_data['Train_TP'] = confusion_train[1][1]\n",
    "nn_data['Train_FP'] = confusion_train[0][1]\n",
    "nn_data['Train_FN'] = confusion_train[1][0]\n",
    "\n",
    "nn_data['Test_TN'] = confusion_test[0][0]\n",
    "nn_data['Test_TP'] = confusion_test[1][1]\n",
    "nn_data['Test_FP'] = confusion_test[0][1]\n",
    "nn_data['Test_FN'] = confusion_test[1][0]\n",
    "\n",
    "nn_data = pd.DataFrame([nn_data])\n",
    "\n",
    "best_models_df = pd.concat([best_models_df, nn_data])\n",
    "\n",
    "#np.array(nn_data['Layers'])\n",
    "\n",
    "best_models_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
