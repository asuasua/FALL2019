{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EXT_SOURCE_3</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_MON</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_QRT</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_YEAR</th>\n",
       "      <th>OBS_30_CNT_SOCIAL_CIRCLE</th>\n",
       "      <th>DEF_30_CNT_SOCIAL_CIRCLE</th>\n",
       "      <th>OBS_60_CNT_SOCIAL_CIRCLE</th>\n",
       "      <th>DEF_60_CNT_SOCIAL_CIRCLE</th>\n",
       "      <th>EXT_SOURCE_2</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>...</th>\n",
       "      <th>REGION_RATING_CLIENT</th>\n",
       "      <th>REGION_RATING_CLIENT_W_CITY</th>\n",
       "      <th>WEEKDAY_APPR_PROCESS_START</th>\n",
       "      <th>HOUR_APPR_PROCESS_START</th>\n",
       "      <th>REG_REGION_NOT_LIVE_REGION</th>\n",
       "      <th>REG_REGION_NOT_WORK_REGION</th>\n",
       "      <th>LIVE_REGION_NOT_WORK_REGION</th>\n",
       "      <th>REG_CITY_NOT_LIVE_CITY</th>\n",
       "      <th>REG_CITY_NOT_WORK_CITY</th>\n",
       "      <th>LIVE_CITY_NOT_WORK_CITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.905793</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.351241</td>\n",
       "      <td>-0.527764</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>WEDNESDAY</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.123108</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.197991</td>\n",
       "      <td>-1.109049</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>MONDAY</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.567098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.868560</td>\n",
       "      <td>-0.249232</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>WEDNESDAY</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.095792</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.086863</td>\n",
       "      <td>2.281779</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>SUNDAY</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.153597</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.035459</td>\n",
       "      <td>2.645082</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>MONDAY</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   EXT_SOURCE_3  AMT_REQ_CREDIT_BUREAU_MON  AMT_REQ_CREDIT_BUREAU_QRT  \\\n",
       "0     -1.905793                        0.0                        0.0   \n",
       "1      1.123108                        0.0                        0.0   \n",
       "2      0.567098                        0.0                        1.0   \n",
       "3     -0.095792                        1.0                        1.0   \n",
       "4      0.153597                        0.0                        0.0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_YEAR  OBS_30_CNT_SOCIAL_CIRCLE  \\\n",
       "0                         1.0                       2.0   \n",
       "1                         0.0                       0.0   \n",
       "2                         1.0                       0.0   \n",
       "3                         2.0                       1.0   \n",
       "4                         0.0                       2.0   \n",
       "\n",
       "   DEF_30_CNT_SOCIAL_CIRCLE  OBS_60_CNT_SOCIAL_CIRCLE  \\\n",
       "0                       2.0                       2.0   \n",
       "1                       0.0                       0.0   \n",
       "2                       0.0                       0.0   \n",
       "3                       0.0                       1.0   \n",
       "4                       0.0                       2.0   \n",
       "\n",
       "   DEF_60_CNT_SOCIAL_CIRCLE  EXT_SOURCE_2  AMT_GOODS_PRICE  ...  \\\n",
       "0                       2.0     -1.351241        -0.527764  ...   \n",
       "1                       0.0      0.197991        -1.109049  ...   \n",
       "2                       0.0     -0.868560        -0.249232  ...   \n",
       "3                       0.0      1.086863         2.281779  ...   \n",
       "4                       0.0      1.035459         2.645082  ...   \n",
       "\n",
       "   REGION_RATING_CLIENT  REGION_RATING_CLIENT_W_CITY  \\\n",
       "0                     2                            2   \n",
       "1                     2                            2   \n",
       "2                     2                            2   \n",
       "3                     2                            2   \n",
       "4                     3                            3   \n",
       "\n",
       "   WEEKDAY_APPR_PROCESS_START  HOUR_APPR_PROCESS_START  \\\n",
       "0                   WEDNESDAY                       10   \n",
       "1                      MONDAY                        9   \n",
       "2                   WEDNESDAY                       16   \n",
       "3                      SUNDAY                       16   \n",
       "4                      MONDAY                       16   \n",
       "\n",
       "  REG_REGION_NOT_LIVE_REGION REG_REGION_NOT_WORK_REGION  \\\n",
       "0                          0                          0   \n",
       "1                          0                          0   \n",
       "2                          0                          0   \n",
       "3                          0                          0   \n",
       "4                          0                          0   \n",
       "\n",
       "   LIVE_REGION_NOT_WORK_REGION  REG_CITY_NOT_LIVE_CITY  \\\n",
       "0                            0                       0   \n",
       "1                            0                       0   \n",
       "2                            0                       0   \n",
       "3                            0                       0   \n",
       "4                            0                       0   \n",
       "\n",
       "   REG_CITY_NOT_WORK_CITY  LIVE_CITY_NOT_WORK_CITY  \n",
       "0                       0                        0  \n",
       "1                       0                        0  \n",
       "2                       0                        0  \n",
       "3                       0                        0  \n",
       "4                       1                        1  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#os.chdir('C:\\\\Users\\\\A Sua\\\\Documents\\\\FIU\\\\Fall2019\\\\ML\\\\project')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"impot data select features\"\"\"\n",
    "data = pd.read_csv(\"application_training_data_import.csv\")\n",
    "features = data.columns\n",
    "s=data.isna().sum()\n",
    "null = pd.read_csv('Null_Entries.csv')\n",
    "d = null[null['flag']==0]\n",
    "columns = d['colname']\n",
    "new_data = data[columns]\n",
    "\n",
    "\n",
    "\n",
    "home_loans = new_data[new_data.isnull().any(axis=1) == False]\n",
    "home_loans = home_loans.reset_index()\n",
    "home_loans = home_loans.drop(columns = 'index')\n",
    "\n",
    "#print(home_loans.shape)\n",
    "\n",
    "\n",
    "cols_to_remove = ['SK_ID_CURR',\n",
    "'FLAG_DOCUMENT_2','FLAG_DOCUMENT_3','FLAG_DOCUMENT_4','FLAG_DOCUMENT_5',\n",
    "'FLAG_DOCUMENT_6','FLAG_DOCUMENT_7','FLAG_DOCUMENT_8',\n",
    "'FLAG_DOCUMENT_9','FLAG_DOCUMENT_10','FLAG_DOCUMENT_11','FLAG_DOCUMENT_12',\n",
    "'FLAG_DOCUMENT_13','FLAG_DOCUMENT_14','FLAG_DOCUMENT_15','FLAG_DOCUMENT_16',\n",
    "'FLAG_DOCUMENT_17','FLAG_DOCUMENT_18','FLAG_DOCUMENT_19','FLAG_DOCUMENT_20','FLAG_DOCUMENT_21',\n",
    "'AMT_REQ_CREDIT_BUREAU_HOUR',\n",
    "'AMT_REQ_CREDIT_BUREAU_DAY',\n",
    "'AMT_REQ_CREDIT_BUREAU_WEEK',\n",
    "'FLAG_CONT_MOBILE']\n",
    "\n",
    "\n",
    "cols_to_normalize = ['EXT_SOURCE_3',\n",
    "'EXT_SOURCE_2','AMT_GOODS_PRICE',\n",
    "'AMT_ANNUITY','DAYS_LAST_PHONE_CHANGE', 'AMT_INCOME_TOTAL',\n",
    "'AMT_CREDIT','REGION_POPULATION_RELATIVE',\n",
    "'DAYS_BIRTH',\n",
    "'DAYS_EMPLOYED',\n",
    "'DAYS_REGISTRATION',\n",
    "'DAYS_ID_PUBLISH']\n",
    "\n",
    "\n",
    "\"\"\"drop columns and add normalized columns\"\"\"\n",
    "home_loans_df = home_loans.drop(cols_to_remove, axis=1)\n",
    "## substitute the normalized columns\n",
    "df_to_normalize = home_loans_df[cols_to_normalize]\n",
    "norm_values = df_to_normalize.values\n",
    "\n",
    "\"\"\"normalize values\"\"\"\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(norm_values)\n",
    "norm_values= scaler.transform(norm_values)\n",
    "norm_values\n",
    "\n",
    "\"\"\"re-impute into data\"\"\"\n",
    "home_loans_df[cols_to_normalize] = norm_values\n",
    "home_loans_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shaped (28850, 67)\n",
      "(9616, 67)\n",
      "Testing data shaped (61333, 67)\n",
      "cross validation shapes\n",
      "(19233, 67)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Set the indices\"\"\"\n",
    "n0 = home_loans_df[home_loans_df['TARGET']==0].shape[0]\n",
    "#test set percentage size \n",
    "test_per = 0.0758\n",
    "n1 = home_loans_df[home_loans_df['TARGET']==1].shape[0]\n",
    "extra = 3000\n",
    "\n",
    "train1_index = 14425\n",
    "train0_index = int(train1_index)\n",
    "\n",
    "test1_index = int(n1 - train1_index)\n",
    "test0_index = int(test1_index/test_per)\n",
    "#test0_index = test0_index+test1_index\n",
    "# print(test1_index)\n",
    "# print(test0_index)\n",
    "\n",
    "\"\"\"Take all TARGET Home Loans\"\"\"\n",
    "home_loans = shuffle(home_loans_df,random_state=0)\n",
    "home_loans0 = home_loans[home_loans['TARGET']==0].iloc[:108200, :]\n",
    "home_loans1 = home_loans[home_loans['TARGET']==1].iloc[:, :]\n",
    "\n",
    "# home_loans_test0 = home_loans[home_loans['TARGET']==0].iloc[10000:101000, :]\n",
    "# home_loans_test1 = home_loans[home_loans['TARGET']==1].iloc[:, :]\n",
    "\n",
    "reduced_loans = pd.concat([home_loans0, home_loans1], axis=0)\n",
    "reduced_loans = shuffle(reduced_loans,random_state=0)\n",
    "\n",
    "\n",
    "reduced_loans = pd.get_dummies(reduced_loans)\n",
    "\n",
    "\n",
    "##########\n",
    "################\n",
    "########---Split data into Test-Train Split\n",
    "####################\n",
    "########################\n",
    "\"\"\"split dataset into training set and cross validation\"\"\"\n",
    "\"\"\"training data will have 50/50 0/1 class split\"\"\"\n",
    "train_df0 = reduced_loans[reduced_loans['TARGET']==0].iloc[:train0_index, :]\n",
    "train_df1 = reduced_loans[reduced_loans['TARGET']==1].iloc[:train1_index, :]\n",
    "\n",
    "train = pd.concat([train_df0, train_df1])\n",
    "train = shuffle(train,random_state=43)\n",
    "\n",
    "\n",
    "\"\"\"Test dataframe will have 90/10 0/1 class split\"\"\"\n",
    "test_df0 = reduced_loans[reduced_loans['TARGET']==0].iloc[test0_index:, :]\n",
    "test_df1 = reduced_loans[reduced_loans['TARGET']==1].iloc[test1_index:, :]\n",
    "\n",
    "test = pd.concat([test_df0, test_df1])\n",
    "test = shuffle(test,random_state=57)\n",
    "\n",
    "del reduced_loans, home_loans, home_loans0, home_loans1\n",
    "######\n",
    "#########\n",
    "#######-----split up the y variables\n",
    "##########\n",
    "\n",
    "\n",
    "\"\"\"separate out the target variable\"\"\"\n",
    "train_target = train['TARGET']\n",
    "train = train.drop(columns = 'TARGET')\n",
    "train = pd.get_dummies(train) #One Hot Encoding\n",
    "# train = train.drop(columns='NAME_INCOME_TYPE_Unemployed')\n",
    "# train = train.drop(columns='GENDER_CODE_XNA')\n",
    "\n",
    "test_target = test['TARGET']\n",
    "test = test.drop(columns = 'TARGET')\n",
    "test = pd.get_dummies(test) #One Hot Encoding\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"break up the data\"\"\"\n",
    "X_train, y_train = np.matrix(train), np.array(train_target)\n",
    "X_test, y_test = np.matrix(test), np.array(test_target)\n",
    "\n",
    "#######\n",
    "#######---Create cross validation sets-------######\n",
    "########\n",
    "#########\n",
    "\n",
    "traincv1 = X_train[:int(np.floor((len(train_target)/3))), :]\n",
    "target1 = y_train[:int(np.floor((len(train_target)/3)))]\n",
    "\n",
    "traincv2 = X_train[int(np.floor((len(train_target)/3))):int(np.floor((len(train_target)*2/3))), :]\n",
    "target2 = y_train[int(np.floor((len(train_target)/3))):int(np.floor((len(train_target)*2/3)))]\n",
    "\n",
    "\n",
    "traincv3 = X_train[int(np.floor((len(train_target)*2/3))):, :]\n",
    "target3 = y_train[int(np.floor((len(train_target)*2/3))):]\n",
    "\n",
    "\n",
    "\"\"\"add test dataset\"\"\"\n",
    "# test = X[int(np.floor((len(reduced_loans)*3/4))):, :]\n",
    "# target_test = y[int(np.floor((len(reduced_loans)*3/4))):]\n",
    "\n",
    "\n",
    "\"\"\"Trainig \"\"\"\n",
    "print('Training data shaped {}'.format(X_train.shape))\n",
    "print(traincv1.shape)\n",
    "\n",
    "print('Testing data shaped {}'.format(X_test.shape))\n",
    "\n",
    "\n",
    "\"\"\"make pairs of datasets\"\"\"\n",
    "CV12 = np.concatenate([traincv1, traincv2])\n",
    "y12 = np.concatenate([target1, target2])\n",
    "\n",
    "CV13 = np.concatenate([traincv1, traincv3])\n",
    "y13 = np.concatenate([target1, target3])\n",
    "\n",
    "CV23 = np.concatenate([traincv2, traincv3])\n",
    "y23 = np.concatenate([target2, target3])\n",
    "\n",
    "print('cross validation shapes')\n",
    "print(CV12.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"define some functions\"\"\"\n",
    "def train_neural_network(nn, learning_rate, epochs, train_tensor, target_tensor): \n",
    "    \"\"\"declare name, input neural_network\"\"\"\n",
    "    neural_network = nn\n",
    "    nn_name = nn.name\n",
    "    optimizer = optim.Adam(neural_network.parameters(), lr=learning_rate) ## Adam optimizer with parameters and learning rate\n",
    "    n_layers = int(str(neural_network).split('fc')[-1].split(')')[0])\n",
    "    EPOCHS = epochs\n",
    "    \n",
    "    XX = torch.tensor(train_tensor).float()\n",
    "    target = torch.tensor(target_tensor).long()\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        XX = XX\n",
    "        target = target\n",
    "        \n",
    "        neural_network.zero_grad()\n",
    "        output = neural_network.forward(XX)\n",
    "        predicted = output.float()\n",
    "        \n",
    "        #print(predicted.shape, target.shape)\n",
    "        loss = F.cross_entropy(predicted, target)\n",
    "        #loss = F.nll_loss(output, y) ## calculate loss function with MSE or cross_entropy\n",
    "        #F.cross_entropy\n",
    "        #  F.leaky_relu\n",
    "        loss.backward() ## do the backprop\n",
    "        optimizer.step() ## update the weights\n",
    "        if epoch % 1000 == 0:\n",
    "            print('epoch {} had loss {}'.format(epoch, loss))\n",
    "\n",
    "    print('final loss was {}'.format(loss))\n",
    "    \n",
    "    nn_data = {'NN': nn_name, 'LR': learning_rate, 'Layers': 0, 'Total_Layers': n_layers, 'Epochs': EPOCHS}\n",
    "#     nn_data['NN'] = nn_name\n",
    "#     nn_data['LR'] = learning_rate\n",
    "#     nn_data['Total_Layers']= n_layers\n",
    "\n",
    "    layer_list = np.array([[]])\n",
    "    for hidden_layer_index in range(n_layers): \n",
    "        arc = str(neural_network).split('fc')[1:][hidden_layer_index].split(',')\n",
    "        in_layer = int(arc[0].split(': ')[1].split('=')[1])\n",
    "        out_layer = int(arc[1].split('=')[1])\n",
    "        layer_values = np.array([in_layer, out_layer])\n",
    "        layer_list = np.append(arr=[layer_list], values=[layer_values])\n",
    "\n",
    "    nn_data['Layers'] = layer_list    \n",
    "    \n",
    "    \n",
    "    return neural_network, nn_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "##########\n",
    "###------------########\n",
    "######-----------------##\n",
    "#####---------------------######\n",
    "def test_scores(neural_network, X_train, y_train, X_test, y_test):\n",
    "    total_train = 0\n",
    "    correct_train = 0\n",
    "    total_test = 0\n",
    "    correct_test = 0\n",
    "    nn_output_train = []\n",
    "    nn_output_test = []\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        XX_train = torch.tensor(X_train).float()\n",
    "        XX_test = torch.tensor(X_test).float()    \n",
    "        #target = torch.tensor(target_tensor).long() \n",
    "\n",
    "        #yy = y_train\n",
    "        output_train = neural_network.forward(XX_train)\n",
    "        output_test = neural_network.forward(XX_test)\n",
    "\n",
    "        for idx, i in enumerate(output_train):\n",
    "    #         print(int(torch.argmax(i)))\n",
    "    #         print(type(torch.argmax(i)))\n",
    "    #        print(type(output_train))\n",
    "            nn_output_train.append(int(torch.argmax(i)))\n",
    "            if torch.argmax(i) == y_train[idx]:\n",
    "                correct_train += 1 ## check the predicted value with the actual value\n",
    "            total_train += 1\n",
    "\n",
    "        for idx, i in enumerate(output_test):\n",
    "            nn_output_test.append(int(torch.argmax(i)))\n",
    "            if torch.argmax(i) == y_test[idx]:\n",
    "                correct_test += 1 ## check the predicted value with the actual value\n",
    "            total_test += 1\n",
    "            \n",
    "    \"\"\"Generate confusion matrices\"\"\"\n",
    "    y_scores_train = np.array(nn_output_train)\n",
    "    y_scores_test = np.array(nn_output_test)\n",
    "    confusion_train = confusion_matrix(y_train, y_scores_train)\n",
    "    confusion_test = confusion_matrix(y_test, y_scores_test)\n",
    "\n",
    "    #print('Trained')\n",
    "    train_accuracy = (confusion_train[0][0]+confusion_train[1][1])/len(y_train)\n",
    "    train_recall = confusion_train[1][1]/(confusion_train[1][0]+confusion_train[1][1])\n",
    "    train_precision = confusion_train[1][1]/(confusion_train[0][1]+confusion_train[1][1])                                             \n",
    "    test_accuracy = (confusion_test[0][0]+confusion_test[1][1])/len(y_test)\n",
    "    test_recall = confusion_test[1][1]/(confusion_test[1][0]+confusion_test[1][1])\n",
    "    test_precision = confusion_test[1][1]/(confusion_test[0][1]+confusion_test[1][1])\n",
    "                                                                                    \n",
    "#     print('Train Accuracy: {}'.format(train_accuracy))\n",
    "#     print('Train Recall: {}'.format(train_recall))\n",
    "#     print('Train Precision: {}'.format(train_precision))\n",
    "#     print(confusion_train)\n",
    "\n",
    "#     print('Test Accuracy: {}'.format(test_accuracy))\n",
    "#     print('Test Recall: {}'.format(test_recall))\n",
    "#     print('Test Precision: {}'.format(test_precision))\n",
    "    #print(confusion_test)        \n",
    "    return_dic = {'Train_Size': len(y_train), 'Test_Size': len(y_test), \n",
    "                  'Train_Accuracy': train_accuracy, 'Train_Recall': train_recall, 'Train_Precision': train_precision,\n",
    "                 'Test_Accuracy': test_accuracy, 'Test_Recall': test_recall, 'Test_Precision': test_precision}\n",
    "    \n",
    "    print('{} was tested'.format(neural_network.name))\n",
    "                                           \n",
    "    return confusion_train, confusion_test, return_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"in order to make a neural network, we need to create an object belonging to the NEURAL NETWORK class\"\"\"\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, name=None):\n",
    "#         \"\"\"inherit NN methods, attributes, functions, etc..., without this we can't use it\"\"\"\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=1*X_train.shape[1], out_features=256) ## (each input is a 2x1 vector) \n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=256) ## (layer1 output, layer 2 output)        \n",
    "        self.fc3 = nn.Linear(in_features=256, out_features=64) ## (layer2 output, layer 3 output)\n",
    "        self.fc4 = nn.Linear(in_features=64, out_features=2) ## (layer 3 output, layer 4 output)\n",
    "        self.name = name\n",
    "        self.train_size = len(y_train)\n",
    "        self.test_size = len(y_test)\n",
    "        print('the nnn name is {}'.format(self.name))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "#         x = F.relu(self.fc1(x)) ## pass vector into first layer\n",
    "#         x = F.relu(self.fc2(x)) ## 2nd layer\n",
    "#         x = F.relu(self.fc3(x)) ### 3rd layer\n",
    "        #x = F.relu\n",
    "        x = F.leaky_relu(self.fc1(x), inplace=True, negative_slope=0.001)\n",
    "        x = F.leaky_relu(self.fc2(x), inplace=True, negative_slope=0.001)\n",
    "        x = F.leaky_relu(self.fc3(x), inplace=True, negative_slope=0.85)\n",
    "        x = self.fc4(x) ### and then run softmax regression on the final layer to estimate 0,...,9 \n",
    "        y = F.softmax(x, dim=1)\n",
    "        #y = F.log_softmax(x, dim=1)\n",
    "        return  y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and perform 3-fold Cross Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the nnn name is NN12\n",
      "epoch 0 had loss 0.6930288672447205\n",
      "final loss was 0.44372084736824036\n",
      "NN12 was tested\n",
      "the nnn name is NN13\n",
      "epoch 0 had loss 0.5475044846534729\n",
      "final loss was 0.4868224263191223\n",
      "NN12 was tested\n",
      "the nnn name is NN23\n",
      "epoch 0 had loss 0.6947623491287231\n",
      "final loss was 0.4494827091693878\n",
      "NN23 was tested\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.005\n",
    "epoch_size = 200\n",
    "\n",
    "\n",
    "\"\"\"Train 12\"\"\"\n",
    "nn12 = Net(name='NN12')\n",
    "nn12, c12_dic = train_neural_network(nn=nn12, learning_rate=alpha, epochs=epoch_size, train_tensor=CV12, target_tensor=y12)\n",
    "c12_train, c12_test, c12_scores = test_scores(neural_network=nn12, X_train=CV12, y_train=y12, X_test=traincv3, y_test=target3)\n",
    "\n",
    "\"\"\"Train 13\"\"\"\n",
    "nn13 = Net(name='NN13')    \n",
    "nn13, c13_dic = train_neural_network(nn=nn12, learning_rate=alpha, epochs=epoch_size, train_tensor=CV13, target_tensor=y13)\n",
    "c13_train, c13_test, c13_scores = test_scores(neural_network=nn13, X_train=CV13, y_train=y13, X_test=traincv2, y_test=target2)\n",
    "\n",
    "\"\"\"Train 23\"\"\"\n",
    "nn23 = Net(name='NN23')\n",
    "nn23, c23_dic = train_neural_network(nn=nn23, learning_rate=alpha, epochs=epoch_size, train_tensor=CV23, target_tensor=y23)\n",
    "c23_train, c23_test, c23_scores = test_scores(neural_network=nn23, X_train=CV23, y_train=y23, X_test=traincv1, y_test=target1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Train_Size': 19233,\n",
       " 'Test_Size': 9617,\n",
       " 'Train_Accuracy': 0.8730827224042012,\n",
       " 'Train_Recall': 0.8721804511278195,\n",
       " 'Train_Precision': 0.872818476329815,\n",
       " 'Test_Accuracy': 0.6525943641468234,\n",
       " 'Test_Recall': 0.6463188286244587,\n",
       " 'Test_Precision': 0.6584033613445378}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c12_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Train_Size': 19233,\n",
       " 'Test_Size': 9617,\n",
       " 'Train_Accuracy': 0.8266521083554308,\n",
       " 'Train_Recall': 0.8124412900532304,\n",
       " 'Train_Precision': 0.8351035296641991,\n",
       " 'Test_Accuracy': 0.7356764063637309,\n",
       " 'Test_Recall': 0.7293559042113955,\n",
       " 'Test_Precision': 0.7416036943744753}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c13_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Train_Size': 19234,\n",
       " 'Test_Size': 9616,\n",
       " 'Train_Accuracy': 0.8682541333056046,\n",
       " 'Train_Recall': 0.8712472918601052,\n",
       " 'Train_Precision': 0.8678450313431302,\n",
       " 'Test_Accuracy': 0.6589018302828619,\n",
       " 'Test_Recall': 0.6390532544378699,\n",
       " 'Test_Precision': 0.6579634464751958}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c23_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue training Top Performing model on Full Training Data & Test on Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 had loss 0.46859070658683777\n",
      "final loss was 0.4621872007846832\n",
      "NN12 was tested\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Train_Size': 28850,\n",
       " 'Test_Size': 61333,\n",
       " 'Train_Accuracy': 0.8512305025996534,\n",
       " 'Train_Recall': 0.8465857885615251,\n",
       " 'Train_Precision': 0.8545238261843118,\n",
       " 'Test_Accuracy': 0.6853243767629172,\n",
       " 'Test_Recall': 0.7889081455805893,\n",
       " 'Test_Precision': 0.41179663470237016}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Train Best\"\"\"\n",
    "\"\"\"The best performing NN was the one trained on CV 1,3\"\"\"\n",
    "best_nn = nn13\n",
    "best_nn, best_dic = train_neural_network(nn=best_nn, learning_rate=alpha, epochs=epoch_size, train_tensor=X_train, target_tensor=y_train)\n",
    "cbest_train, cbest_test, best_scores = test_scores(neural_network=best_nn, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test)\n",
    "\n",
    "best_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
