{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EXT_SOURCE_3</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_MON</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_QRT</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_YEAR</th>\n",
       "      <th>OBS_30_CNT_SOCIAL_CIRCLE</th>\n",
       "      <th>DEF_30_CNT_SOCIAL_CIRCLE</th>\n",
       "      <th>OBS_60_CNT_SOCIAL_CIRCLE</th>\n",
       "      <th>DEF_60_CNT_SOCIAL_CIRCLE</th>\n",
       "      <th>EXT_SOURCE_2</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>...</th>\n",
       "      <th>REGION_RATING_CLIENT</th>\n",
       "      <th>REGION_RATING_CLIENT_W_CITY</th>\n",
       "      <th>WEEKDAY_APPR_PROCESS_START</th>\n",
       "      <th>HOUR_APPR_PROCESS_START</th>\n",
       "      <th>REG_REGION_NOT_LIVE_REGION</th>\n",
       "      <th>REG_REGION_NOT_WORK_REGION</th>\n",
       "      <th>LIVE_REGION_NOT_WORK_REGION</th>\n",
       "      <th>REG_CITY_NOT_LIVE_CITY</th>\n",
       "      <th>REG_CITY_NOT_WORK_CITY</th>\n",
       "      <th>LIVE_CITY_NOT_WORK_CITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.905793</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.351241</td>\n",
       "      <td>-0.527764</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>WEDNESDAY</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.123108</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.197991</td>\n",
       "      <td>-1.109049</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>MONDAY</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.567098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.868560</td>\n",
       "      <td>-0.249232</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>WEDNESDAY</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.095792</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.086863</td>\n",
       "      <td>2.281779</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>SUNDAY</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.153597</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.035459</td>\n",
       "      <td>2.645082</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>MONDAY</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   EXT_SOURCE_3  AMT_REQ_CREDIT_BUREAU_MON  AMT_REQ_CREDIT_BUREAU_QRT  \\\n",
       "0     -1.905793                        0.0                        0.0   \n",
       "1      1.123108                        0.0                        0.0   \n",
       "2      0.567098                        0.0                        1.0   \n",
       "3     -0.095792                        1.0                        1.0   \n",
       "4      0.153597                        0.0                        0.0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_YEAR  OBS_30_CNT_SOCIAL_CIRCLE  \\\n",
       "0                         1.0                       2.0   \n",
       "1                         0.0                       0.0   \n",
       "2                         1.0                       0.0   \n",
       "3                         2.0                       1.0   \n",
       "4                         0.0                       2.0   \n",
       "\n",
       "   DEF_30_CNT_SOCIAL_CIRCLE  OBS_60_CNT_SOCIAL_CIRCLE  \\\n",
       "0                       2.0                       2.0   \n",
       "1                       0.0                       0.0   \n",
       "2                       0.0                       0.0   \n",
       "3                       0.0                       1.0   \n",
       "4                       0.0                       2.0   \n",
       "\n",
       "   DEF_60_CNT_SOCIAL_CIRCLE  EXT_SOURCE_2  AMT_GOODS_PRICE  ...  \\\n",
       "0                       2.0     -1.351241        -0.527764  ...   \n",
       "1                       0.0      0.197991        -1.109049  ...   \n",
       "2                       0.0     -0.868560        -0.249232  ...   \n",
       "3                       0.0      1.086863         2.281779  ...   \n",
       "4                       0.0      1.035459         2.645082  ...   \n",
       "\n",
       "   REGION_RATING_CLIENT  REGION_RATING_CLIENT_W_CITY  \\\n",
       "0                     2                            2   \n",
       "1                     2                            2   \n",
       "2                     2                            2   \n",
       "3                     2                            2   \n",
       "4                     3                            3   \n",
       "\n",
       "   WEEKDAY_APPR_PROCESS_START  HOUR_APPR_PROCESS_START  \\\n",
       "0                   WEDNESDAY                       10   \n",
       "1                      MONDAY                        9   \n",
       "2                   WEDNESDAY                       16   \n",
       "3                      SUNDAY                       16   \n",
       "4                      MONDAY                       16   \n",
       "\n",
       "  REG_REGION_NOT_LIVE_REGION REG_REGION_NOT_WORK_REGION  \\\n",
       "0                          0                          0   \n",
       "1                          0                          0   \n",
       "2                          0                          0   \n",
       "3                          0                          0   \n",
       "4                          0                          0   \n",
       "\n",
       "   LIVE_REGION_NOT_WORK_REGION  REG_CITY_NOT_LIVE_CITY  \\\n",
       "0                            0                       0   \n",
       "1                            0                       0   \n",
       "2                            0                       0   \n",
       "3                            0                       0   \n",
       "4                            0                       0   \n",
       "\n",
       "   REG_CITY_NOT_WORK_CITY  LIVE_CITY_NOT_WORK_CITY  \n",
       "0                       0                        0  \n",
       "1                       0                        0  \n",
       "2                       0                        0  \n",
       "3                       0                        0  \n",
       "4                       1                        1  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"impot data select features\"\"\"\n",
    "data = pd.read_csv(\"application_training_data_import.csv\")\n",
    "features = data.columns\n",
    "s=data.isna().sum()\n",
    "null = pd.read_csv('Null_Entries.csv')\n",
    "d = null[null['flag']==0]\n",
    "columns = d['colname']\n",
    "new_data = data[columns]\n",
    "\n",
    "\n",
    "\n",
    "home_loans = new_data[new_data.isnull().any(axis=1) == False]\n",
    "home_loans = home_loans.reset_index()\n",
    "home_loans = home_loans.drop(columns = 'index')\n",
    "\n",
    "#print(home_loans.shape)\n",
    "\n",
    "\n",
    "cols_to_remove = ['SK_ID_CURR',\n",
    "'FLAG_DOCUMENT_2','FLAG_DOCUMENT_3','FLAG_DOCUMENT_4','FLAG_DOCUMENT_5',\n",
    "'FLAG_DOCUMENT_6','FLAG_DOCUMENT_7','FLAG_DOCUMENT_8',\n",
    "'FLAG_DOCUMENT_9','FLAG_DOCUMENT_10','FLAG_DOCUMENT_11','FLAG_DOCUMENT_12',\n",
    "'FLAG_DOCUMENT_13','FLAG_DOCUMENT_14','FLAG_DOCUMENT_15','FLAG_DOCUMENT_16',\n",
    "'FLAG_DOCUMENT_17','FLAG_DOCUMENT_18','FLAG_DOCUMENT_19','FLAG_DOCUMENT_20','FLAG_DOCUMENT_21',\n",
    "'AMT_REQ_CREDIT_BUREAU_HOUR',\n",
    "'AMT_REQ_CREDIT_BUREAU_DAY',\n",
    "'AMT_REQ_CREDIT_BUREAU_WEEK',\n",
    "'FLAG_CONT_MOBILE']\n",
    "\n",
    "\n",
    "cols_to_normalize = ['EXT_SOURCE_3',\n",
    "'EXT_SOURCE_2','AMT_GOODS_PRICE',\n",
    "'AMT_ANNUITY','DAYS_LAST_PHONE_CHANGE', 'AMT_INCOME_TOTAL',\n",
    "'AMT_CREDIT','REGION_POPULATION_RELATIVE',\n",
    "'DAYS_BIRTH',\n",
    "'DAYS_EMPLOYED',\n",
    "'DAYS_REGISTRATION',\n",
    "'DAYS_ID_PUBLISH']\n",
    "\n",
    "\n",
    "\"\"\"drop columns and add normalized columns\"\"\"\n",
    "home_loans_df = home_loans.drop(cols_to_remove, axis=1)\n",
    "## substitute the normalized columns\n",
    "df_to_normalize = home_loans_df[cols_to_normalize]\n",
    "norm_values = df_to_normalize.values\n",
    "\n",
    "\"\"\"normalize values\"\"\"\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(norm_values)\n",
    "norm_values= scaler.transform(norm_values)\n",
    "norm_values\n",
    "\n",
    "\"\"\"re-impute into data\"\"\"\n",
    "home_loans_df[cols_to_normalize] = norm_values\n",
    "home_loans_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shaped (28850, 67)\n",
      "(9616, 67)\n",
      "Testing data shaped (61333, 67)\n",
      "cross validation shapes\n",
      "(19233, 67)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Set the indices\"\"\"\n",
    "n0 = home_loans_df[home_loans_df['TARGET']==0].shape[0]\n",
    "#test set percentage size \n",
    "test_per = 0.0758\n",
    "n1 = home_loans_df[home_loans_df['TARGET']==1].shape[0]\n",
    "extra = 3000\n",
    "\n",
    "train1_index = 14425\n",
    "train0_index = int(train1_index)\n",
    "\n",
    "test1_index = int(n1 - train1_index)\n",
    "test0_index = int(test1_index/test_per)\n",
    "#test0_index = test0_index+test1_index\n",
    "# print(test1_index)\n",
    "# print(test0_index)\n",
    "\n",
    "\"\"\"Take all TARGET Home Loans\"\"\"\n",
    "home_loans = shuffle(home_loans_df,random_state=0)\n",
    "home_loans0 = home_loans[home_loans['TARGET']==0].iloc[:108200, :]\n",
    "home_loans1 = home_loans[home_loans['TARGET']==1].iloc[:, :]\n",
    "\n",
    "# home_loans_test0 = home_loans[home_loans['TARGET']==0].iloc[10000:101000, :]\n",
    "# home_loans_test1 = home_loans[home_loans['TARGET']==1].iloc[:, :]\n",
    "\n",
    "reduced_loans = pd.concat([home_loans0, home_loans1], axis=0)\n",
    "reduced_loans = shuffle(reduced_loans,random_state=0)\n",
    "\n",
    "\n",
    "reduced_loans = pd.get_dummies(reduced_loans)\n",
    "\n",
    "\n",
    "##########\n",
    "################\n",
    "########---Split data into Test-Train Split\n",
    "####################\n",
    "########################\n",
    "\"\"\"split dataset into training set and cross validation\"\"\"\n",
    "\"\"\"training data will have 50/50 0/1 class split\"\"\"\n",
    "train_df0 = reduced_loans[reduced_loans['TARGET']==0].iloc[:train0_index, :]\n",
    "train_df1 = reduced_loans[reduced_loans['TARGET']==1].iloc[:train1_index, :]\n",
    "\n",
    "train = pd.concat([train_df0, train_df1])\n",
    "train = shuffle(train,random_state=43)\n",
    "\n",
    "\n",
    "\"\"\"Test dataframe will have 90/10 0/1 class split\"\"\"\n",
    "test_df0 = reduced_loans[reduced_loans['TARGET']==0].iloc[test0_index:, :]\n",
    "test_df1 = reduced_loans[reduced_loans['TARGET']==1].iloc[test1_index:, :]\n",
    "\n",
    "test = pd.concat([test_df0, test_df1])\n",
    "test = shuffle(test,random_state=57)\n",
    "\n",
    "del reduced_loans, home_loans, home_loans0, home_loans1\n",
    "######\n",
    "#########\n",
    "#######-----split up the y variables\n",
    "##########\n",
    "\n",
    "\n",
    "\"\"\"separate out the target variable\"\"\"\n",
    "train_target = train['TARGET']\n",
    "train = train.drop(columns = 'TARGET')\n",
    "train = pd.get_dummies(train) #One Hot Encoding\n",
    "# train = train.drop(columns='NAME_INCOME_TYPE_Unemployed')\n",
    "# train = train.drop(columns='GENDER_CODE_XNA')\n",
    "\n",
    "test_target = test['TARGET']\n",
    "test = test.drop(columns = 'TARGET')\n",
    "test = pd.get_dummies(test) #One Hot Encoding\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"break up the data\"\"\"\n",
    "X_train, y_train = np.matrix(train), np.array(train_target)\n",
    "X_test, y_test = np.matrix(test), np.array(test_target)\n",
    "\n",
    "#######\n",
    "#######---Create cross validation sets-------######\n",
    "########\n",
    "#########\n",
    "\n",
    "traincv1 = X_train[:int(np.floor((len(train_target)/3))), :]\n",
    "target1 = y_train[:int(np.floor((len(train_target)/3)))]\n",
    "\n",
    "traincv2 = X_train[int(np.floor((len(train_target)/3))):int(np.floor((len(train_target)*2/3))), :]\n",
    "target2 = y_train[int(np.floor((len(train_target)/3))):int(np.floor((len(train_target)*2/3)))]\n",
    "\n",
    "\n",
    "traincv3 = X_train[int(np.floor((len(train_target)*2/3))):, :]\n",
    "target3 = y_train[int(np.floor((len(train_target)*2/3))):]\n",
    "\n",
    "\n",
    "\"\"\"add test dataset\"\"\"\n",
    "# test = X[int(np.floor((len(reduced_loans)*3/4))):, :]\n",
    "# target_test = y[int(np.floor((len(reduced_loans)*3/4))):]\n",
    "\n",
    "\n",
    "\"\"\"Trainig \"\"\"\n",
    "print('Training data shaped {}'.format(X_train.shape))\n",
    "print(traincv1.shape)\n",
    "\n",
    "print('Testing data shaped {}'.format(X_test.shape))\n",
    "\n",
    "\n",
    "\"\"\"make pairs of datasets\"\"\"\n",
    "CV12 = np.concatenate([traincv1, traincv2])\n",
    "y12 = np.concatenate([target1, target2])\n",
    "\n",
    "CV13 = np.concatenate([traincv1, traincv3])\n",
    "y13 = np.concatenate([target1, target3])\n",
    "\n",
    "CV23 = np.concatenate([traincv2, traincv3])\n",
    "y23 = np.concatenate([target2, target3])\n",
    "\n",
    "print('cross validation shapes')\n",
    "print(CV12.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on 1, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training on 1,2\n",
      "Trained on 1,2\n",
      "Train Confusion Matrix with accuracy 0.9103623979618364\n",
      "Train Recall: 0.9065371762740184\n",
      "[[8828  829]\n",
      " [ 895 8681]]\n",
      "\n",
      "Test Confusion Matrix with accuracy 0.6705833419985442\n",
      "Test Recall: 0.657042689214271\n",
      "[[3263 1505]\n",
      " [1663 3186]]\n"
     ]
    }
   ],
   "source": [
    "support_vector_machine12 = SVC(kernel='rbf', gamma=0.1, C=1., random_state=None)\n",
    "support_vector_machine12.fit(CV12, y12)\n",
    "print('finished training on 1,2')\n",
    "\n",
    "#y_scores = clf.decision_function(X_test)\n",
    "y_scores_train = support_vector_machine12.predict(CV12)\n",
    "y_scores_test = support_vector_machine12.predict(traincv3)\n",
    "\n",
    "confusion_train = confusion_matrix(y12, y_scores_train)\n",
    "confusion_test = confusion_matrix(target3, y_scores_test)\n",
    "\n",
    "\n",
    "print('Trained on 1,2')\n",
    "print('Train Confusion Matrix with accuracy {}'.format((confusion_train[0][0]+confusion_train[1][1])/len(y12)))\n",
    "print('Train Recall: {}'.format((confusion_train[1][1]/(confusion_train[1][0]+confusion_train[1][1]))))\n",
    "print(confusion_train)\n",
    "print('')\n",
    "print('Test Confusion Matrix with accuracy {}'.format((confusion_test[0][0]+confusion_test[1][1])/len(target3)))\n",
    "print('Test Recall: {}'.format((confusion_test[1][1]/(confusion_test[1][0]+confusion_test[1][1]))))\n",
    "print(confusion_test)\n",
    "#precision, recall, thresholds = precision_recall_curve(y_test, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training on 1,3\n",
      "Trained on 1,3\n",
      "Train Confusion Matrix with accuracy 0.9084906150886497\n",
      "Train Recall: 0.9041853668719341\n",
      "[[8810  842]\n",
      " [ 918 8663]]\n",
      "\n",
      "Test Confusion Matrix with accuracy 0.6716231673078923\n",
      "Test Recall: 0.6717588769611891\n",
      "[[3205 1568]\n",
      " [1590 3254]]\n"
     ]
    }
   ],
   "source": [
    "support_vector_machine13 = SVC(kernel='rbf', gamma=0.1, C=1., random_state=None)\n",
    "support_vector_machine13.fit(CV13, y13)\n",
    "print('finished training on 1,3')\n",
    "\n",
    "#y_scores = clf.decision_function(X_test)\n",
    "y_scores_train = support_vector_machine13.predict(CV13) \n",
    "y_scores_test = support_vector_machine13.predict(traincv2)\n",
    "\n",
    "confusion_train = confusion_matrix(y13, y_scores_train)\n",
    "confusion_test = confusion_matrix(target2, y_scores_test)\n",
    "\n",
    "\n",
    "print('Trained on 1,3')\n",
    "print('Train Confusion Matrix with accuracy {}'.format((confusion_train[0][0]+confusion_train[1][1])/len(y13)))\n",
    "print('Train Recall: {}'.format((confusion_train[1][1]/(confusion_train[1][0]+confusion_train[1][1]))))\n",
    "print(confusion_train)\n",
    "print('')\n",
    "print('Test Confusion Matrix with accuracy {}'.format((confusion_test[0][0]+confusion_test[1][1])/len(target2)))\n",
    "print('Test Recall: {}'.format((confusion_test[1][1]/(confusion_test[1][0]+confusion_test[1][1]))))\n",
    "print(confusion_test)\n",
    "#precision, recall, thresholds = precision_recall_curve(y_test, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training on 2,3\n",
      "Trained on 2,3\n",
      "Train Confusion Matrix with accuracy 0.9118228137672871\n",
      "Train Recall: 0.9072526565562777\n",
      "[[8744  797]\n",
      " [ 899 8794]]\n",
      "\n",
      "Test Confusion Matrix with accuracy 0.6644134775374376\n",
      "Test Recall: 0.6464497041420119\n",
      "[[3330 1554]\n",
      " [1673 3059]]\n"
     ]
    }
   ],
   "source": [
    "support_vector_machine23 = SVC(kernel='rbf', gamma=0.1, C=1., random_state=None)\n",
    "support_vector_machine23.fit(CV23, y23)\n",
    "print('finished training on 2,3')\n",
    "#y_scores = clf.decision_function(X_test)\n",
    "y_scores_train = support_vector_machine23.predict(CV23)\n",
    "y_scores_test = support_vector_machine23.predict(traincv1)\n",
    "\n",
    "confusion_train = confusion_matrix(y23, y_scores_train)\n",
    "confusion_test = confusion_matrix(target1, y_scores_test)\n",
    "\n",
    "\n",
    "print('Trained on 2,3')\n",
    "print('Train Confusion Matrix with accuracy {}'.format((confusion_train[0][0]+confusion_train[1][1])/len(y23)))\n",
    "print('Train Recall: {}'.format((confusion_train[1][1]/(confusion_train[1][0]+confusion_train[1][1]))))\n",
    "print(confusion_train)\n",
    "print('')\n",
    "print('Test Confusion Matrix with accuracy {}'.format((confusion_test[0][0]+confusion_test[1][1])/len(target1)))\n",
    "print('Test Recall: {}'.format((confusion_test[1][1]/(confusion_test[1][0]+confusion_test[1][1]))))\n",
    "print(confusion_test)\n",
    "#precision, recall, thresholds = precision_recall_curve(y_test, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Best Model and Test on Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training on FULL Training Data\n",
      "Trained on FULL Training Data\n",
      "Train Confusion Matrix with accuracy 0.8984055459272097\n",
      "Train Recall: 0.8944887348353553\n",
      "[[13016  1409]\n",
      " [ 1522 12903]]\n",
      "\n",
      "Test Confusion Matrix with accuracy 0.7104495133125723\n",
      "Test Recall: 0.8205892547660312\n",
      "[[31737 15171]\n",
      " [ 2588 11837]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"After Eyeballing the model with the best Recall\"\"\"\n",
    "support_vector_machine_best = support_vector_machine13\n",
    "support_vector_machine_best.fit(X_train, y_train)\n",
    "print('finished training on FULL Training Data')\n",
    "#y_scores = clf.decision_function(X_test)\n",
    "y_scores_train = support_vector_machine_best.predict(X_train)\n",
    "y_scores_test = support_vector_machine_best.predict(X_test)\n",
    "\n",
    "confusion_train = confusion_matrix(y_train, y_scores_train)\n",
    "confusion_test = confusion_matrix(y_test, y_scores_test)\n",
    "\n",
    "\n",
    "print('Trained on FULL Training Data')\n",
    "print('Train Confusion Matrix with accuracy {}'.format((confusion_train[0][0]+confusion_train[1][1])/len(y_train)))\n",
    "print('Train Recall: {}'.format((confusion_train[1][1]/(confusion_train[1][0]+confusion_train[1][1]))))\n",
    "print(confusion_train)\n",
    "print('')\n",
    "print('Test Confusion Matrix with accuracy {}'.format((confusion_test[0][0]+confusion_test[1][1])/len(y_test)))\n",
    "print('Test Recall: {}'.format((confusion_test[1][1]/(confusion_test[1][0]+confusion_test[1][1]))))\n",
    "print(confusion_test)\n",
    "#precision, recall, thresholds = precision_recall_curve(y_test, y_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
